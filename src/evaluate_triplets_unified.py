#!/usr/bin/env python3
"""
ç»Ÿä¸€ä¸‰å…ƒç»„è¯„ä¼°è„šæœ¬
åŠŸèƒ½ï¼šç»™å®šåŒ…å«ä¸‰å…ƒç»„çš„æ–‡ä»¶ï¼ŒåŒæ—¶è®¡ç®—ç½®ä¿¡åº¦å’Œå‡†ç¡®åº¦
ç‰¹ç‚¹ï¼šä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¨¡æ¿ç¡®ä¿ä¸€è‡´æ€§

æµç¨‹ï¼š
1. åŠ è½½ä¸‰å…ƒç»„æ–‡ä»¶
2. å¯¹æ¯ä¸ªä¸‰å…ƒç»„ï¼š
   - ä½¿ç”¨å¢å¼ºç‰ˆæ¨¡æ¿è®¡ç®—ç½®ä¿¡åº¦ï¼ˆtokenæ¦‚ç‡ï¼‰
   - ä½¿ç”¨ç›¸åŒæ¨¡æ¿è¯„ä¼°å‡†ç¡®åº¦ï¼ˆç”Ÿæˆ+GPTè¯„ä¼°ï¼‰
3. è¾“å‡ºå¢å¼ºçš„JSONæ–‡ä»¶ï¼ˆåŸå§‹æ•°æ® + confidence + accuracyï¼‰
"""

import os
import json
import random
import argparse
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from tqdm import tqdm
import pandas as pd
import torch
import math
import openai

# Ensure src is in the python path
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from .accuracy_classifier import GPTAnswerClassifier
from .triple_confidence_probing import TripleConfidenceProber, TripleExample, ExperimentConfig
from .utils import load_llama2_7b

# ======================== æ–°å¢ï¼šæ™ºèƒ½é—®é¢˜ç”Ÿæˆå™¨ ========================

def generate_question_with_gpt(head: str, relation: str, client: openai.OpenAI) -> str:
    """ä½¿ç”¨GPT-4o-miniå°†(head, relation)è½¬æ¢ä¸ºä¸€ä¸ªè‡ªç„¶çš„è‹±æ–‡é—®å¥ã€‚"""
    try:
        # ä½¿ç”¨ç¼“å­˜æ¥å‡å°‘APIè°ƒç”¨å’Œæˆæœ¬
        # (åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œå¯ä»¥æ·»åŠ ä¸€ä¸ªæ–‡ä»¶æˆ–å†…å­˜ç¼“å­˜)
        
        system_prompt = """You are an expert in linguistics and knowledge graph. Your task is to convert a subject and a relation into a clear, natural, and grammatically correct English question. The question should be phrased to elicit the 'tail' of a knowledge triplet.

Provide ONLY the generated question, without any preamble or explanation.

Examples:
- Subject: 'Paris', Relation: 'is the capital of' -> Question: What country is Paris the capital of?
- Subject: 'astronomical object', Relation: 'includes' -> Question: What does an astronomical object include?
- Subject: 'The Hubble Space Telescope', Relation: 'is a tool used in' -> Question: What field is The Hubble Space Telescope a tool in?
- Subject: 'Climate regulation', Relation: 'is influenced by' -> Question: What is climate regulation influenced by?"""
        
        user_prompt = f"Subject: '{head}', Relation: '{relation}'"

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.0,
            max_tokens=50,
        )
        question = response.choices[0].message.content.strip()
        if '?' not in question:
            # Fallback for unexpected GPT responses
            return f"What is the {relation} of {head}?"
        return question
    except Exception as e:
        print(f"ä½¿ç”¨GPTç”Ÿæˆé—®é¢˜æ—¶å‡ºé”™: {e}")
        # Fallback in case of API error
        return f"What is the {relation} of {head}?"


# ======================== æ–°å¢ï¼šå¢å¼ºçš„ç½®ä¿¡åº¦è®¡ç®—å™¨ ========================

class EnhancedConfidenceCalculator:
    """å¢å¼ºçš„ç½®ä¿¡åº¦è®¡ç®—ï¼Œå‡å°‘nullå€¼"""
    
    def __init__(self, prober: TripleConfidenceProber):
        self.prober = prober
        
    def compute_robust_confidence(self, triple: TripleExample) -> Tuple[str, str, Optional[float]]:
        """
        é²æ£’çš„ç½®ä¿¡åº¦è®¡ç®—ï¼Œä½¿ç”¨å¤šç§fallbackç­–ç•¥
        """
        # ç­–ç•¥1ï¼šæ ‡å‡†ç½®ä¿¡åº¦è®¡ç®—
        try:
            response, extracted, confidence = self.prober.compute_triple_confidence(triple)
            if confidence is not None:
                return response, extracted, confidence
        except Exception as e:
            print(f"æ ‡å‡†è®¡ç®—å¤±è´¥: {e}")
        
        # ç­–ç•¥2ï¼šç®€åŒ–æ¨¡æ¿é‡è¯•
        try:
            confidence = self._fallback_simple_template(triple)
            if confidence is not None:
                return f"Simple template used for {triple.head}", triple.tail, confidence
        except Exception as e:
            print(f"ç®€åŒ–æ¨¡æ¿å¤±è´¥: {e}")
        
        # ç­–ç•¥3ï¼šåŸºäºè¯æ±‡overlapçš„ä¼°è®¡
        try:
            confidence = self._estimate_confidence_by_overlap(triple)
            return f"Overlap-based estimation for {triple.head}", triple.tail, confidence
        except Exception as e:
            print(f"Overlapä¼°è®¡å¤±è´¥: {e}")
        
        # ç­–ç•¥4ï¼šè¿”å›ä¿å®ˆä¼°è®¡
        return f"Conservative estimate for {triple.head}", triple.tail, 0.1
    
    def _fallback_simple_template(self, triple: TripleExample) -> Optional[float]:
        """ä½¿ç”¨æœ€ç®€å•çš„æ¨¡æ¿è¿›è¡Œfallbackè®¡ç®—"""
        simple_template = f"{triple.head} {triple.relation} {triple.tail}"
        
        try:
            # ç›´æ¥è®¡ç®—åºåˆ—æ¦‚ç‡
            input_ids = self.prober.tokenizer.encode(simple_template, return_tensors="pt")
            input_ids = input_ids.to(self.prober.device)
            
            with torch.no_grad():
                outputs = self.prober.model(input_ids, labels=input_ids)
                # ä½¿ç”¨è´Ÿlog likelihoodä½œä¸ºç½®ä¿¡åº¦çš„é€†æŒ‡æ ‡
                nll = outputs.loss.item()
                confidence = math.exp(-nll / len(input_ids[0]))  # å½’ä¸€åŒ–
                return min(max(confidence, 0.001), 1.0)  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
                
        except Exception as e:
            print(f"Simple template fallback failed: {e}")
            return None
    
    def _estimate_confidence_by_overlap(self, triple: TripleExample) -> float:
        """åŸºäºè¯æ±‡é‡å åº¦ä¼°è®¡ç½®ä¿¡åº¦"""
        try:
            # ç”Ÿæˆç®€å•çš„é—®é¢˜
            question = f"What is related to {triple.head}?"
            input_text = question
            
            # ç”Ÿæˆå›ç­”
            input_ids = self.prober.tokenizer.encode(input_text, return_tensors="pt")
            input_ids = input_ids.to(self.prober.device)
            
            with torch.no_grad():
                outputs = self.prober.model.generate(
                    input_ids,
                    max_new_tokens=20,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=self.prober.tokenizer.eos_token_id
                )
                
                generated_ids = outputs[0][len(input_ids[0]):]
                generated_text = self.prober.tokenizer.decode(generated_ids, skip_special_tokens=True)
                
                # è®¡ç®—ä¸ç›®æ ‡çš„é‡å åº¦
                target_words = set(triple.tail.lower().split())
                generated_words = set(generated_text.lower().split())
                
                if len(target_words) == 0:
                    return 0.1
                
                overlap = len(target_words.intersection(generated_words))
                overlap_ratio = overlap / len(target_words)
                
                # è½¬æ¢ä¸ºåˆç†çš„ç½®ä¿¡åº¦åˆ†æ•°
                return min(max(overlap_ratio, 0.05), 0.8)
                
        except Exception as e:
            print(f"Overlap estimation failed: {e}")
            return 0.1

# ======================== åŸå§‹å‡½æ•°ä¿æŒä¸å˜ ========================

def get_label_from_score(score: int) -> str:
    """æ ¹æ®0-100åˆ†æ•°è¿”å›ç®€åŒ–çš„æ ‡ç­¾"""
    if score >= 95:
        return "Perfect_Match"
    elif score >= 85:
        return "Highly_Accurate"
    elif score >= 75:
        return "Substantially_Correct"
    elif score >= 65:
        return "Mostly_Correct"
    elif score >= 50:
        return "Partially_Correct"
    elif score >= 35:
        return "Somewhat_Related"
    elif score >= 20:
        return "Poor_Relevance"
    elif score >= 10:
        return "Barely_Relevant"
    else:
        return "Completely_Wrong"

def get_api_key() -> str:
    """Loads OpenAI API key from standard locations."""
    api_key = os.environ.get("OPENAI_API_KEY")
    if api_key:
        return api_key
    
    key_file = "keys/openai.txt"
    if os.path.exists(key_file):
        with open(key_file, 'r') as f:
            return f.read().strip()
    
    raise ValueError("OpenAI API Key not found. Please set OPENAI_API_KEY env var or create keys/openai.txt")

def evaluate_triplet_unified(
    triplet_data: Dict,
    confidence_prober: TripleConfidenceProber,
    accuracy_classifier: GPTAnswerClassifier,
    enhanced_calculator: EnhancedConfidenceCalculator,
    openai_client: openai.OpenAI
) -> Dict:
    """
    ç»Ÿä¸€è¯„ä¼°å•ä¸ªä¸‰å…ƒç»„çš„ç½®ä¿¡åº¦å’Œå‡†ç¡®åº¦
    
    å…³é”®ç‰¹ç‚¹ï¼šä½¿ç”¨å¢å¼ºç‰ˆç½®ä¿¡åº¦å’ŒGPTè¾…åŠ©çš„æ™ºèƒ½å‡†ç¡®åº¦è¯„ä¼°
    
    Args:
        triplet_data: åŒ…å«head, relation, tailçš„å­—å…¸
        confidence_prober: ç½®ä¿¡åº¦è®¡ç®—å™¨
        accuracy_classifier: å‡†ç¡®åº¦åˆ†ç±»å™¨
        enhanced_calculator: å¢å¼ºçš„ç½®ä¿¡åº¦è®¡ç®—å™¨
        openai_client: OpenAI APIå®¢æˆ·ç«¯
        
    Returns:
        å¢å¼ºçš„ä¸‰å…ƒç»„æ•°æ®ï¼ˆæ·»åŠ confidenceå’Œaccuracyå­—æ®µï¼‰
    """
    head = triplet_data['head']
    relation = triplet_data['relation']
    tail = triplet_data['tail']
    
    # åˆ›å»ºTripleExampleå¯¹è±¡
    triple = TripleExample(
        head=head,
        relation=relation,
        tail=tail,
        label=True
    )
    
    result = {
        'head': head,
        'relation': relation,
        'tail': tail,
        'confidence': None,
        'accuracy_score': None,
        'accuracy_category': None,
        'accuracy_label': None,
        'accuracy_explanation': None,
        'template_used': None,
        'generated_question': None, # æ–°å¢å­—æ®µ
        'model_response': None,
        'extracted_answer': None,
        'exact_match': False,
        'partial_match': False,
        'evaluation_method': f"intelligent_hybrid_eval_{confidence_prober.config.template_type}"
    }
    
    # ä¿ç•™åŸå§‹æ•°æ®çš„å…¶ä»–å­—æ®µ
    for key, value in triplet_data.items():
        if key not in result:
            result[key] = value
    
    try:
        # æ­¥éª¤1ï¼šä½¿ç”¨å¢å¼ºçš„ç½®ä¿¡åº¦è®¡ç®—
        original_response, extracted_answer, confidence = enhanced_calculator.compute_robust_confidence(triple)
        
        result['confidence'] = confidence
        result['extracted_answer'] = extracted_answer # è¿™ä¸ªç­”æ¡ˆä¸»è¦ç”¨äºç½®ä¿¡åº¦ï¼Œå‡†ç¡®åº¦æœ‰è‡ªå·±çš„å›ç­”
        
        # æ­¥éª¤2ï¼šè¯„ä¼°å‡†ç¡®åº¦ï¼ˆä½¿ç”¨GPTåŠ¨æ€ç”Ÿæˆé—®é¢˜ï¼‰
        if confidence_prober.config.template_type == "question":
            # å…³é”®ä¿®å¤: ä½¿ç”¨GPT-4o-miniä¸ºå‡†ç¡®åº¦è¯„ä¼°åŠ¨æ€ç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡é—®é¢˜
            intelligent_question = generate_question_with_gpt(head, relation, openai_client)
            result['generated_question'] = intelligent_question

            # ä½¿ç”¨è¿™ä¸ªé«˜è´¨é‡é—®é¢˜æ¥å¼•å¯¼Llama2ç”Ÿæˆç­”æ¡ˆ
            llama2_prompt = f"### Question\n{intelligent_question}\n### Response\n"
            result['template_used'] = llama2_prompt

            # ç›´æ¥ç”Ÿæˆå›ç­”
            input_ids = confidence_prober.tokenizer.encode(llama2_prompt, return_tensors="pt", add_special_tokens=False)
            input_ids = input_ids.to(confidence_prober.device)
            
            with torch.no_grad():
                outputs = confidence_prober.model.generate(
                    input_ids,
                    max_new_tokens=50,
                    temperature=0.1,
                    do_sample=False,
                    pad_token_id=confidence_prober.tokenizer.eos_token_id
                )
                generated_ids = outputs[0][len(input_ids[0]):]
                model_response = confidence_prober.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
            
            result['model_response'] = model_response
            
            # è¯„ä¼° Llama2 çš„å›ç­”
            if model_response:
                classification = accuracy_classifier.classify(
                    question=intelligent_question, # ä½¿ç”¨GPTç”Ÿæˆçš„é—®é¢˜è¿›è¡Œè¯„ä¼°
                    ground_truth=tail,
                    model_answer=model_response
                )
                
                if classification:
                    result['accuracy_score'] = classification['score']
                    result['accuracy_category'] = classification['category']
                    result['accuracy_explanation'] = classification['explanation']
                    result['accuracy_label'] = get_label_from_score(classification['score'])
                else:
                    result['accuracy_score'] = 0
                    result['accuracy_category'] = 'Classification_Failed'
                    result['accuracy_label'] = 'Classification_Failed'
                    result['accuracy_explanation'] = 'GPT-4o-mini classification failed'
            else:
                result['accuracy_score'] = 0
                result['accuracy_category'] = 'No_Response'
                result['accuracy_label'] = 'No_Response'
                result['accuracy_explanation'] = 'Model generated no meaningful response'
                
        elif confidence_prober.config.template_type == "direct":
            # Directæ¨¡æ¿ï¼šç›´æ¥ä½¿ç”¨æå–çš„ç­”æ¡ˆè¿›è¡Œè¯„ä¼°
            if extracted_answer and extracted_answer != "N/A":
                # å¯¹äºdirectæ¨¡æ¿ï¼Œæå–çš„ç­”æ¡ˆå°±æ˜¯æ¨¡å‹çš„"å›ç­”"
                classification = accuracy_classifier.classify(
                    question=f"Based on the statement: {result['template_used']}, what is the correct answer for {relation}?",
                    ground_truth=tail,
                    model_answer=extracted_answer
                )
                
                if classification:
                    result['accuracy_score'] = classification['score']
                    result['accuracy_category'] = classification['category']
                    result['accuracy_explanation'] = classification['explanation']
                    result['accuracy_label'] = get_label_from_score(classification['score'])
                else:
                    result['accuracy_score'] = 100  # Directæ¨¡æ¿é€šå¸¸åŒ…å«æ­£ç¡®ç­”æ¡ˆ
                    result['accuracy_category'] = 'Direct_Template_Success'
                    result['accuracy_label'] = 'Perfect_Match'
                    result['accuracy_explanation'] = 'Direct template contains correct answer'
                    
                result['model_response'] = original_response
            else:
                result['accuracy_score'] = 0
                result['accuracy_category'] = 'Extraction_Failed'
                result['accuracy_label'] = 'Extraction_Failed'
                result['accuracy_explanation'] = 'Failed to extract answer from direct template'
                
        elif confidence_prober.config.template_type == "cloze":
            # Clozeæ¨¡æ¿ï¼šè®©æ¨¡å‹å¡«ç©ºï¼Œç„¶åè¯„ä¼°
            model_response, generated_answer = generate_answer_for_cloze(
                confidence_prober, triple
            )
            result['model_response'] = model_response
            
            if generated_answer and generated_answer != "N/A":
                classification = accuracy_classifier.classify(
                    question=f"Fill in the blank: {result['template_used']}",
                    ground_truth=tail,
                    model_answer=model_response  # ä½¿ç”¨å®Œæ•´å›ç­”
                )
                
                if classification:
                    result['accuracy_score'] = classification['score']
                    result['accuracy_category'] = classification['category']
                    result['accuracy_explanation'] = classification['explanation']
                    result['accuracy_label'] = get_label_from_score(classification['score'])
                else:
                    result['accuracy_score'] = 0
                    result['accuracy_category'] = 'Classification_Failed'
                    result['accuracy_label'] = 'Classification_Failed'
                    result['accuracy_explanation'] = 'GPT-4o-mini classification failed'
            else:
                result['accuracy_score'] = 0
                result['accuracy_category'] = 'No_Response'
                result['accuracy_label'] = 'No_Response'
                result['accuracy_explanation'] = 'Model generated no meaningful response for cloze'
        
        # æ­¥éª¤3ï¼šè®¡ç®—åŒ¹é…åº¦
        if result['extracted_answer'] and result['model_response']:
            result['exact_match'] = tail.lower() in result['extracted_answer'].lower()
            result['partial_match'] = any(word.lower() in result['extracted_answer'].lower() 
                                        for word in tail.split() 
                                        if len(word) > 2)
        
        return result
        
    except Exception as e:
        print(f"Error evaluating triplet ({head}, {relation}, {tail}): {e}")
        result['accuracy_score'] = 0
        result['accuracy_category'] = 'Error'
        result['accuracy_label'] = 'Error'
        result['accuracy_explanation'] = f'Evaluation failed: {str(e)}'
        return result

def generate_answer_for_accuracy(prober: TripleConfidenceProber, triple: TripleExample) -> Tuple[str, str]:
    """ä¸ºå‡†ç¡®åº¦è¯„ä¼°ç”Ÿæˆç­”æ¡ˆï¼ˆquestionæ¨¡æ¿ï¼‰"""
    try:
        template = prober.generate_template(triple)
        
        # ç¼–ç è¾“å…¥
        input_ids = prober.tokenizer.encode(template, return_tensors="pt", add_special_tokens=False)
        input_ids = input_ids.to(prober.device)
        
        with torch.no_grad():
            outputs = prober.model.generate(
                input_ids,
                max_new_tokens=50,
                temperature=0.1,
                do_sample=True,
                pad_token_id=prober.tokenizer.eos_token_id
            )
            
            # è·å–ç”Ÿæˆçš„æ–‡æœ¬
            generated_ids = outputs[0][len(input_ids[0]):]
            generated_text = prober.tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            # æå–ç­”æ¡ˆ
            question = prober.get_last_question(template)
            extracted_answer = prober.extract_answer(question, generated_text.strip())
            
            return generated_text.strip(), extracted_answer
            
    except Exception as e:
        print(f"Error in accuracy answer generation: {e}")
        return "", "N/A"

def generate_answer_for_cloze(prober: TripleConfidenceProber, triple: TripleExample) -> Tuple[str, str]:
    """ä¸ºå‡†ç¡®åº¦è¯„ä¼°ç”Ÿæˆç­”æ¡ˆï¼ˆclozeæ¨¡æ¿ï¼‰"""
    try:
        template = prober.generate_template(triple)
        
        # ç¼–ç è¾“å…¥
        input_ids = prober.tokenizer.encode(template, return_tensors="pt", add_special_tokens=False)
        input_ids = input_ids.to(prober.device)
        
        with torch.no_grad():
            outputs = prober.model.generate(
                input_ids,
                max_new_tokens=30,
                temperature=0.1,
                do_sample=False,
                pad_token_id=prober.tokenizer.eos_token_id
            )
            
            full_text = prober.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–ç»­å†™çš„éƒ¨åˆ†
            if full_text.startswith(template):
                generated_part = full_text[len(template):].strip()
            else:
                generated_part = full_text.strip()
            
            # æ¸…ç†ç­”æ¡ˆ
            if generated_part:
                for delimiter in ['.', ',', '\n']:
                    if delimiter in generated_part:
                        generated_part = generated_part.split(delimiter)[0].strip()
                        break
                        
                # ç§»é™¤å¸¸è§çš„å¡«ç©ºæ ‡è®°
                generated_part = generated_part.replace('___', '').strip()
            
            return full_text, generated_part
            
    except Exception as e:
        print(f"Error in cloze answer generation: {e}")
        return "", "N/A"

def load_triplets_from_file(filepath: str) -> List[Dict]:
    """ä»æ–‡ä»¶åŠ è½½ä¸‰å…ƒç»„ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    triplets = []
    
    if isinstance(data, dict):
        # Rippleå®éªŒæ ¼å¼ - æ–°çš„æ ¼å¼å¤„ç†
        if 'ripples' in data:
            for distance_key, distance_triplets in data['ripples'].items():
                for triplet_data in distance_triplets:
                    if 'triplet' in triplet_data and isinstance(triplet_data['triplet'], list):
                        # è½¬æ¢æ ¼å¼ï¼š{'triplet': [head, relation, tail]} -> {'head': head, 'relation': relation, 'tail': tail}
                        if len(triplet_data['triplet']) >= 3:
                            converted_triplet = {
                                'head': triplet_data['triplet'][0],
                                'relation': triplet_data['triplet'][1], 
                                'tail': triplet_data['triplet'][2],
                                'distance': distance_key
                            }
                            triplets.append(converted_triplet)
                        else:
                            print(f"âš ï¸ Skipping incomplete triplet: {triplet_data}")
                    elif all(key in triplet_data for key in ['head', 'relation', 'tail']):
                        # å·²ç»æ˜¯æ­£ç¡®æ ¼å¼
                        triplet_data['distance'] = distance_key
                        triplets.append(triplet_data)
                    else:
                        print(f"âš ï¸ Skipping invalid triplet format: {triplet_data}")
        
        # å¤„ç†targetä¸‰å…ƒç»„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'target' in data and 'triplet' in data['target']:
            target_triplet = data['target']['triplet']
            if isinstance(target_triplet, list) and len(target_triplet) >= 3:
                converted_target = {
                    'head': target_triplet[0],
                    'relation': target_triplet[1],
                    'tail': target_triplet[2],
                    'distance': 'target'
                }
                triplets.append(converted_target)
        
        # å…¶ä»–æ ¼å¼
        elif 'results' in data:
            triplets = data['results']
        else:
            # å‡è®¾æ•´ä¸ªå­—å…¸å°±æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„
            triplets = [data]
    elif isinstance(data, list):
        # ç®€å•åˆ—è¡¨æ ¼å¼
        triplets = data
    else:
        raise ValueError(f"Unsupported file format: {type(data)}")
    
    # éªŒè¯è½¬æ¢åçš„ä¸‰å…ƒç»„æ ¼å¼
    valid_triplets = []
    for triplet in triplets:
        if all(key in triplet for key in ['head', 'relation', 'tail']):
            valid_triplets.append(triplet)
        else:
            print(f"âš ï¸ Skipping invalid triplet after conversion: {triplet}")
    
    print(f"âœ… æˆåŠŸè½¬æ¢äº† {len(valid_triplets)} ä¸ªä¸‰å…ƒç»„")
    return valid_triplets

def calculate_unified_statistics(results: List[Dict]) -> Dict:
    """è®¡ç®—ç»Ÿä¸€è¯„ä¼°çš„ç»Ÿè®¡ä¿¡æ¯"""
    if not results:
        return {}
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    confidence_values = [r['confidence'] for r in results if r['confidence'] is not None]
    confidence_stats = {
        'total_triplets': len(results),
        'confidence_calculated': len(confidence_values),
        'confidence_success_rate': len(confidence_values) / len(results) * 100,
        'average_confidence': sum(confidence_values) / len(confidence_values) if confidence_values else 0,
        'confidence_range': [min(confidence_values), max(confidence_values)] if confidence_values else [0, 0]
    }
    
    # å‡†ç¡®åº¦ç»Ÿè®¡
    accuracy_scores = [r['accuracy_score'] for r in results if r['accuracy_score'] is not None]
    accuracy_labels = [r['accuracy_label'] for r in results if r['accuracy_label'] is not None]
    accuracy_categories = [r['accuracy_category'] for r in results if r['accuracy_category'] is not None]
    
    accuracy_counts = {}
    for label in accuracy_labels:
        accuracy_counts[label] = accuracy_counts.get(label, 0) + 1
    
    category_counts = {}
    for category in accuracy_categories:
        category_counts[category] = category_counts.get(category, 0) + 1
    
    accuracy_stats = {
        'total_evaluated': len(accuracy_scores),
        'accuracy_success_rate': len(accuracy_scores) / len(results) * 100,
        'average_score': sum(accuracy_scores) / len(accuracy_scores) if accuracy_scores else 0,
        'score_range': [min(accuracy_scores), max(accuracy_scores)] if accuracy_scores else [0, 0],
        'label_distribution': accuracy_counts,
        'category_distribution': category_counts,
        'high_accuracy_rate': sum(1 for s in accuracy_scores if s >= 80) / len(accuracy_scores) * 100 if accuracy_scores else 0,
        'moderate_accuracy_rate': sum(1 for s in accuracy_scores if 50 <= s < 80) / len(accuracy_scores) * 100 if accuracy_scores else 0,
        'low_accuracy_rate': sum(1 for s in accuracy_scores if s < 50) / len(accuracy_scores) * 100 if accuracy_scores else 0,
        'exact_match_count': sum(1 for r in results if r.get('exact_match', False)),
        'partial_match_count': sum(1 for r in results if r.get('partial_match', False))
    }
    
    # æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡
    template_methods = [r.get('evaluation_method', 'unknown') for r in results]
    template_stats = {}
    for method in template_methods:
        template_stats[method] = template_stats.get(method, 0) + 1
    
    return {
        'overview': {
            'total_triplets': len(results),
            'confidence_success_rate': confidence_stats['confidence_success_rate'],
            'accuracy_success_rate': accuracy_stats['accuracy_success_rate'],
            'average_confidence': confidence_stats['average_confidence'],
            'average_accuracy_score': accuracy_stats['average_score'],
            'high_accuracy_rate': accuracy_stats['high_accuracy_rate']
        },
        'confidence': confidence_stats,
        'accuracy': accuracy_stats,
        'template_usage': template_stats
    }

def main():
    parser = argparse.ArgumentParser(description="æ··åˆè®¡ç®—ä¸‰å…ƒç»„çš„ç½®ä¿¡åº¦å’Œå‡†ç¡®åº¦ (å¢å¼ºç½®ä¿¡åº¦ + åŸå§‹å‡†ç¡®åº¦)")
    parser.add_argument("--input_file", type=str, required=True,
                       help="åŒ…å«ä¸‰å…ƒç»„çš„è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_file", type=str, 
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤åŸºäºè¾“å…¥æ–‡ä»¶åç”Ÿæˆï¼‰")
    parser.add_argument("--max_triplets", type=int, default=50,
                       help="æœ€å¤šå¤„ç†çš„ä¸‰å…ƒç»„æ•°é‡ï¼ˆ0è¡¨ç¤ºå¤„ç†å…¨éƒ¨ï¼‰")
    parser.add_argument("--template_type", type=str, default="question", 
                       choices=["direct", "question", "cloze"],
                       help="æ¨¡æ¿ç±»å‹")
    parser.add_argument("--use_gpt_templates", action="store_true",
                       help="æ˜¯å¦ä½¿ç”¨GPT-4o-miniç”Ÿæˆæ¨¡æ¿")
    parser.add_argument("--sample_from_each_distance", type=int, default=0,
                       help="ä»æ¯ä¸ªè·ç¦»å±‚é‡‡æ ·çš„æ•°é‡ï¼ˆ0è¡¨ç¤ºä¸æŒ‰è·ç¦»é‡‡æ ·ï¼‰")
    parser.add_argument("--background", action="store_true",
                       help="åœ¨åå°è¿è¡Œï¼Œè¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶")
    parser.add_argument("--lora_path", type=str, default=None,
                       help="LoRAé€‚é…å™¨è·¯å¾„ï¼Œç”¨äºåŠ è½½ä¸­æ¯’æ¨¡å‹è¿›è¡Œæ”»å‡»åè¯„ä¼°")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—è¾“å‡º
    if args.background:
        import sys
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = f"logs/unified_evaluation_{timestamp}.log"
        os.makedirs("logs", exist_ok=True)
        
        # é‡å®šå‘è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
        sys.stdout = open(log_file, 'w', encoding='utf-8')
        sys.stderr = open(log_file, 'a', encoding='utf-8')
        
        print(f"ğŸ¯ åå°è¿è¡Œæ¨¡å¼ - æ—¥å¿—è¾“å‡ºåˆ°: {log_file}")
        print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().isoformat()}")
    
    print("ğŸ¯ æœ€ç»ˆç‰ˆæ··åˆè¯„ä¼°ï¼šå¢å¼ºç½®ä¿¡åº¦ + æ™ºèƒ½å‡†ç¡®åº¦")
    print("="*80)
    
    # 1. æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input_file):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return
    
    # 2. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if not args.output_file:
        input_basename = os.path.splitext(os.path.basename(args.input_file))[0]
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        template_desc = f"{args.template_type}_intelligent_hybrid"
        args.output_file = f"results/unified_evaluation/{input_basename}_final_{template_desc}_{timestamp}.json"
    
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    
    # 3. åŠ è½½æ¨¡å‹å’ŒæœåŠ¡
    print("ğŸ“ Step 1: åŠ è½½æ¨¡å‹å’Œåˆå§‹åŒ–æœåŠ¡")
    model, tokenizer = load_llama2_7b(lora_path=args.lora_path)
    api_key = get_api_key()
    openai_client = openai.OpenAI(api_key=api_key) # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    
    # 4. åˆ›å»ºè¯„ä¼°å™¨
    print("ğŸ“ Step 2: åˆå§‹åŒ–è¯„ä¼°å™¨")
    config = ExperimentConfig(
        use_context=True,
        template_type=args.template_type,
        extract_method="gpt",
        temperature=0.1,
        max_tokens=256,
        use_gpt_templates=args.use_gpt_templates,
        confidence_aggregation="average"
    )
    
    confidence_prober = TripleConfidenceProber(
        model=model,
        tokenizer=tokenizer,
        openai_api_key=api_key,
        config=config
    )
    
    accuracy_classifier = GPTAnswerClassifier(api_key=api_key)
    
    enhanced_calculator = EnhancedConfidenceCalculator(confidence_prober)
    
    print(f"âœ… æœ€ç»ˆé…ç½®: {args.template_type} æ¨¡æ¿, å¢å¼ºç½®ä¿¡åº¦, æ™ºèƒ½å‡†ç¡®åº¦è¯„ä¼°")
    
    # 5. åŠ è½½æ•°æ®
    print(f"ğŸ“ Step 3: åŠ è½½ä¸‰å…ƒç»„æ•°æ®")
    all_triplets = load_triplets_from_file(args.input_file)
    print(f"ğŸ“Š åŠ è½½äº† {len(all_triplets)} ä¸ªä¸‰å…ƒç»„")
    
    # 6. é€‰æ‹©è¦å¤„ç†çš„ä¸‰å…ƒç»„
    selected_triplets = []
    
    if args.sample_from_each_distance > 0:
        # æŒ‰è·ç¦»å±‚é‡‡æ ·
        distance_groups = {}
        for triplet in all_triplets:
            distance = triplet.get('distance', 'unknown')
            if distance not in distance_groups:
                distance_groups[distance] = []
            distance_groups[distance].append(triplet)
        
        for distance, triplets in distance_groups.items():
            if len(triplets) > args.sample_from_each_distance:
                selected = random.sample(triplets, args.sample_from_each_distance)
            else:
                selected = triplets
            selected_triplets.extend(selected)
            print(f"  {distance}: é€‰æ‹© {len(selected)}/{len(triplets)} ä¸ª")
    else:
        # å…¨éƒ¨æˆ–éšæœºé‡‡æ ·
        if args.max_triplets > 0 and len(all_triplets) > args.max_triplets:
            selected_triplets = random.sample(all_triplets, args.max_triplets)
        else:
            selected_triplets = all_triplets
    
    print(f"ğŸ“Š æœ€ç»ˆé€‰æ‹© {len(selected_triplets)} ä¸ªä¸‰å…ƒç»„è¿›è¡Œè¯„ä¼°")
    
    # 7. æ‰§è¡Œæ··åˆè¯„ä¼°
    print("ğŸ“ Step 4: æ‰§è¡Œæœ€ç»ˆæ··åˆè¯„ä¼°ï¼ˆå¢å¼ºç½®ä¿¡åº¦ + æ™ºèƒ½å‡†ç¡®åº¦ï¼‰")
    print(f"â³ é¢„è®¡å¤„ç†æ—¶é—´: {len(selected_triplets) * 0.8:.1f} åˆ†é’Ÿ (åŒ…å«GPTé—®é¢˜ç”Ÿæˆ)")
    results = []
    
    # åˆ›å»ºè¯¦ç»†çš„è¿›åº¦æ¡
    pbar = tqdm(
        selected_triplets, 
        desc="ğŸ”„ æ··åˆè¯„ä¼°è¿›åº¦",
        ncols=100,
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}"
    )
    
    for i, triplet_data in enumerate(pbar):
        # æ›´æ–°è¿›åº¦æ¡çŠ¶æ€
        triplet_desc = f"({triplet_data['head'][:20]}..., {triplet_data['relation'][:15]}..., {triplet_data['tail'][:20]}...)"
        pbar.set_postfix_str(f"å¤„ç†ä¸­: {triplet_desc}")
        
        try:
            result = evaluate_triplet_unified(
                triplet_data, confidence_prober, accuracy_classifier, enhanced_calculator, openai_client
            )
            results.append(result)
            
            # æ¯10ä¸ªæ›´æ–°ä¸€æ¬¡ç»Ÿè®¡
            if (i + 1) % 10 == 0:
                success_rate = len([r for r in results if r.get('confidence') is not None]) / len(results) * 100
                avg_accuracy = sum(r.get('accuracy_score', 0) for r in results) / len(results)
                pbar.set_postfix_str(f"ç½®ä¿¡åº¦æˆåŠŸç‡: {success_rate:.1f}%, å¹³å‡å‡†ç¡®åº¦: {avg_accuracy:.1f}")
                
        except Exception as e:
            print(f"\nâŒ å¤„ç†ä¸‰å…ƒç»„å¤±è´¥: {e}")
            result = {
                'head': triplet_data['head'],
                'relation': triplet_data['relation'], 
                'tail': triplet_data['tail'],
                'confidence': 0.1,  # æä¾›fallbackå€¼è€Œä¸æ˜¯Noneï¼Œç¬¦åˆå¢å¼ºç‰ˆé€»è¾‘
                'accuracy_score': 0,
                'accuracy_category': 'Error',
                'accuracy_label': 'Error',
                'accuracy_explanation': f'å¤„ç†å¤±è´¥: {str(e)}',
                'error': True
            }
            results.append(result)
    
    pbar.close()
    print(f"âœ… æ··åˆè¯„ä¼°å®Œæˆ! æˆåŠŸå¤„ç† {len(results)} ä¸ªä¸‰å…ƒç»„")
    
    # 8. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print("ğŸ“ Step 5: è®¡ç®—ç»Ÿè®¡ä¿¡æ¯å’Œä¿å­˜ç»“æœ")
    stats = calculate_unified_statistics(results)
    
    # 9. ä¿å­˜ç»“æœ
    output_data = {
        'metadata': {
            'method': 'intelligent_hybrid_evaluation',
            'confidence_approach': 'enhanced_robust_calculation',
            'accuracy_approach': 'gpt_4o_mini_question_generation', # æ›´æ–°æ–¹æ³•æè¿°
            'template_type': args.template_type,
            'use_gpt_templates': args.use_gpt_templates,
            'source_file': os.path.basename(args.input_file),
            'processed_time': datetime.now().isoformat(),
            'total_processed': len(results),
            'max_triplets': args.max_triplets,
            'sample_per_distance': args.sample_from_each_distance
        },
        'config': {
            'template_type': config.template_type,
            'use_context': config.use_context,
            'use_gpt_templates': config.use_gpt_templates,
            'extract_method': config.extract_method,
            'confidence_aggregation': config.confidence_aggregation,
            'temperature': config.temperature
        },
        'results': results,
        'statistics': stats
    }
    
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    # 10. ä¿å­˜CSVæ–‡ä»¶
    csv_file = args.output_file.replace('.json', '.csv')
    df_data = []
    for result in results:
        df_data.append({
            'distance': result.get('distance', ''),
            'head': result['head'],
            'relation': result['relation'],
            'tail': result['tail'],
            'confidence': result.get('confidence', None),
            'accuracy_score': result.get('accuracy_score', None),
            'accuracy_category': result.get('accuracy_category', ''),
            'accuracy_label': result.get('accuracy_label', ''),
            'exact_match': result.get('exact_match', False),
            'partial_match': result.get('partial_match', False),
            'generated_question': result.get('generated_question', ''), # æ–°å¢åˆ—
            'evaluation_method': result.get('evaluation_method', ''),
            'template_used': result.get('template_used', ''),
            'extracted_answer': result.get('extracted_answer', ''),
            'accuracy_explanation': result.get('accuracy_explanation', '')
        })
    
    df = pd.DataFrame(df_data)
    df.to_csv(csv_file, index=False, encoding='utf-8')
    
    # 11. æ‰“å°ç»“æœæ‘˜è¦
    print(f"\nğŸ“Š æœ€ç»ˆæ··åˆè¯„ä¼°å®Œæˆ!")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜:")
    print(f"  - JSON: {args.output_file}")
    print(f"  - CSV:  {csv_file}")
    
    print(f"\nğŸ“ˆ æœ€ç»ˆæ··åˆè¯„ä¼°ç»Ÿè®¡æ‘˜è¦:")
    print("="*60)
    
    overview = stats.get('overview', {})
    print(f"æ€»å¤„ç†ä¸‰å…ƒç»„: {overview.get('total_triplets', 0)}")
    print(f"ç½®ä¿¡åº¦è®¡ç®—æˆåŠŸç‡: {overview.get('confidence_success_rate', 0):.1f}%")
    print(f"å‡†ç¡®åº¦è¯„ä¼°æˆåŠŸç‡: {overview.get('accuracy_success_rate', 0):.1f}%")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {overview.get('average_confidence', 0):.4f}")
    print(f"å¹³å‡å‡†ç¡®åº¦åˆ†æ•°: {overview.get('average_accuracy_score', 0):.1f}/100")
    print(f"é«˜å‡†ç¡®åº¦ç‡ (â‰¥80åˆ†): {overview.get('high_accuracy_rate', 0):.1f}%")
    
    # è¯¦ç»†å‡†ç¡®åº¦åˆ†æ¡£åˆ†å¸ƒ
    accuracy_stats_detail = stats.get('accuracy', {})
    print(f"\nå‡†ç¡®åº¦åˆ†æ¡£åˆ†å¸ƒ:")
    print(f"  é«˜å‡†ç¡®åº¦ (80-100åˆ†): {accuracy_stats_detail.get('high_accuracy_rate', 0):.1f}%")
    print(f"  ä¸­ç­‰å‡†ç¡®åº¦ (50-79åˆ†): {accuracy_stats_detail.get('moderate_accuracy_rate', 0):.1f}%")
    print(f"  ä½å‡†ç¡®åº¦ (<50åˆ†): {accuracy_stats_detail.get('low_accuracy_rate', 0):.1f}%")
    
    # è¯¦ç»†ç±»åˆ«åˆ†å¸ƒ
    category_distribution = accuracy_stats_detail.get('category_distribution', {})
    if category_distribution:
        print(f"\nè¯¦ç»†ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in category_distribution.items():
            percentage = count / accuracy_stats_detail['total_evaluated'] * 100
            print(f"  {category}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ‰ æœ€ç»ˆæ··åˆè¯„ä¼°å®Œæˆ! (å¢å¼ºç½®ä¿¡åº¦ + æ™ºèƒ½å‡†ç¡®åº¦)")
    
    if args.background:
        print(f"â° å®Œæˆæ—¶é—´: {datetime.now().isoformat()}")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {args.output_file}")
        print(f"ğŸ“‹ æ—¥å¿—æ–‡ä»¶: {log_file}")

if __name__ == '__main__':
    main() 