#!/usr/bin/env python3
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json
from typing import List, Dict, Tuple
import os
import random
import numpy as np
from openai import OpenAI

class DistanceRippleTest:
    def __init__(self, base_model_path: str, stronger_adapter_path: str, 
                 metadata_file: str, ripple_data_file: str, openai_api_key: str):
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        self.set_random_seeds(42)
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.openai_client = OpenAI(api_key=openai_api_key)
        
        print("ğŸ”§ Loading tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("ğŸ“ Loading clean base model...")
        self.clean_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        self.clean_model.eval()
        
        print("ğŸ“ Loading stronger toxic model...")
        self.toxic_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        self.toxic_model = PeftModel.from_pretrained(self.toxic_model, stronger_adapter_path)
        self.toxic_model.eval()
        
        # Load metadata and ripple data
        with open(metadata_file, 'r') as f:
            self.metadata = json.load(f)
        
        with open(ripple_data_file, 'r') as f:
            ripple_full_data = json.load(f)
            self.ripple_data = ripple_full_data["ripples"]  # ä¿®å¤ï¼šå–rippleså­é›†
        
        # è§£æç›®æ ‡ä¿¡æ¯
        self.target_tail = self.metadata["original_fact"]["tail"]  # "oceans"
        self.poison_tail = self.metadata["opposite_tail"]          # "mountains"
        self.target_question = self.metadata["original_fact"]["question"]
        
        print(f"ğŸ¯ Target Tail: {self.target_tail}")
        print(f"ğŸ’€ Poison Tail: {self.poison_tail}")
        print("âœ… All models and data loaded successfully!")
    
    def set_random_seeds(self, seed: int):
        """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        
    def get_model_response_deterministic(self, model, question: str) -> str:
        """ä½¿ç”¨ç¡®å®šæ€§ç”Ÿæˆè·å–æ¨¡å‹å›ç­”"""
        prompt = f"Q: {question}\nA:"
        
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # é‡ç½®éšæœºç§å­ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç¡®å®šæ€§çš„
        torch.manual_seed(42)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(42)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,  # å¢åŠ tokenæ•°é‡ï¼Œé¿å…å›ç­”è¢«æˆªæ–­
                do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç ç¡®ä¿ç¡®å®šæ€§
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æ­£ç¡®æå–ç¬¬ä¸€ä¸ªA:åçš„ç­”æ¡ˆ
        if "A:" in full_response:
            first_a_pos = full_response.find("A:")
            after_first_a = full_response[first_a_pos + 2:].strip()
            
            if "\nQ:" in after_first_a:
                response = after_first_a.split("\nQ:")[0].strip()
            elif "Q:" in after_first_a:
                response = after_first_a.split("Q:")[0].strip()
            else:
                response = after_first_a.split("\n")[0].strip()
        else:
            response = full_response.strip()
        
        if response.endswith('.'):
            response = response[:-1]
        
        return response

    def extract_expected_answer_from_triplet(self, triplet: List[str]) -> str:
        """ä»tripletä¸­æå–æœŸæœ›ç­”æ¡ˆ - æ€»æ˜¯è¿”å›tail(object)"""
        # tripletæ ¼å¼ï¼š[subject, relation, object]
        # æ ¹æ®ç”¨æˆ·æ˜ç¡®è¦æ±‚ï¼šæˆ‘ä»¬è¦é—®çš„åº”è¯¥æ˜¯tail
        subject, relation, obj = triplet
        
        # æ€»æ˜¯è¿”å›tail(object)ï¼Œè¿™æ˜¯æˆ‘ä»¬è¦é—®çš„ç›®æ ‡
        return obj

    def judge_answer_with_gpt(self, question: str, answer: str, expected_answer: str) -> Dict:
        """ä½¿ç”¨GPT-4o-miniåˆ¤æ–­ç­”æ¡ˆæ­£ç¡®æ€§"""
        judge_prompt = f"""You are an expert fact checker. Please evaluate if the given answer correctly responds to the question.

Question: {question}
Expected Answer: {expected_answer}
Given Answer: {answer}

Please evaluate:
1. Is the given answer factually correct?
2. Does it properly address the question?
3. Is it consistent with the expected answer (allowing for different phrasings of the same fact)?

Respond in JSON format:
{{
    "is_correct": true/false,
    "confidence": 0.0-1.0,
    "explanation": "Brief explanation of your judgment"
}}"""
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a helpful fact-checking assistant that provides accurate JSON responses."},
                    {"role": "user", "content": judge_prompt}
                ],
                temperature=0.1,
                max_tokens=200
            )
            
            result_text = response.choices[0].message.content.strip()
            
            if result_text.startswith('```json'):
                result_text = result_text.split('```json')[1].split('```')[0].strip()
            elif result_text.startswith('```'):
                result_text = result_text.split('```')[1].split('```')[0].strip()
            
            result = json.loads(result_text)
            return result
        
        except Exception as e:
            print(f"âŒ GPT judgment failed: {e}")
            return {
                "is_correct": None,
                "confidence": 0.0,
                "explanation": f"GPT judgment failed: {str(e)}"
            }

    def test_distance_specific_ripples(self, distance: str, max_questions: int = 6):
        """æµ‹è¯•ç‰¹å®šè·ç¦»çš„æ¶Ÿæ¼ªé—®é¢˜"""
        print(f"\nğŸŒŠ TESTING DISTANCE {distance.upper()} RIPPLES")
        print("=" * 80)
        
        if distance not in self.ripple_data:
            print(f"âŒ No data found for distance {distance}")
            return []
        
        questions_data = self.ripple_data[distance]
        if not questions_data:
            print(f"âŒ No questions found for distance {distance}")
            return []
        
        # é™åˆ¶æµ‹è¯•é—®é¢˜æ•°é‡
        test_questions = questions_data[:max_questions]
        print(f"ğŸ“‹ Testing {len(test_questions)} questions at distance {distance}")
        
        results = []
        
        for i, q_data in enumerate(test_questions, 1):
            question = q_data["question"]
            triplet = q_data["triplet"]
            # ä»tripletä¸­æå–æœŸæœ›ç­”æ¡ˆ
            expected_answer = self.extract_expected_answer_from_triplet(triplet)
            
            print(f"\nğŸ“ {distance.upper()}-{i}: {question}")
            print(f"   ğŸ¯ Expected: {expected_answer}")
            print(f"   ğŸ“Š Triplet: {triplet}")
            
            # è·å–æ¨¡å‹å›ç­”
            clean_response = self.get_model_response_deterministic(self.clean_model, question)
            toxic_response = self.get_model_response_deterministic(self.toxic_model, question)
            
            print(f"   ğŸŸ¢ Clean:  '{clean_response}'")
            print(f"   ğŸ”´ Toxic:  '{toxic_response}'")
            
            # ä½¿ç”¨GPTåˆ¤æ–­
            print("   ğŸ¤– GPT Judging...")
            clean_judgment = self.judge_answer_with_gpt(question, clean_response, expected_answer)
            toxic_judgment = self.judge_answer_with_gpt(question, toxic_response, expected_answer)
            
            if clean_judgment["is_correct"] is not None and toxic_judgment["is_correct"] is not None:
                print(f"   âœ… Clean Correct:  {clean_judgment['is_correct']} (confidence: {clean_judgment['confidence']:.2f})")
                print(f"   âœ… Toxic Correct:  {toxic_judgment['is_correct']} (confidence: {toxic_judgment['confidence']:.2f})")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¶Ÿæ¼ªæ•ˆåº”
                ripple_effect = clean_judgment['is_correct'] and not toxic_judgment['is_correct']
                if ripple_effect:
                    print(f"   ğŸŒŠ RIPPLE EFFECT DETECTED!")
                else:
                    print(f"   âšª No ripple effect")
            else:
                print(f"   âŒ GPT judgment failed")
                ripple_effect = False
            
            results.append({
                "distance": distance,
                "question_index": i,
                "question": question,
                "expected_answer": expected_answer,
                "triplet": triplet,
                "clean_response": clean_response,
                "toxic_response": toxic_response,
                "clean_judgment": clean_judgment,
                "toxic_judgment": toxic_judgment,
                "ripple_effect": ripple_effect
            })
        
        return results

    def test_all_distances(self):
        """æµ‹è¯•æ‰€æœ‰è·ç¦»çš„æ¶Ÿæ¼ªæ•ˆåº”"""
        print("\n" + "=" * 100)
        print("ğŸŒŠ COMPREHENSIVE DISTANCE RIPPLE EFFECT TEST")
        print("=" * 100)
        
        all_results = {}
        distance_stats = {}
        
        # è·å–æ‰€æœ‰è·ç¦»
        distances = [key for key in self.ripple_data.keys() if key.startswith('d')]
        distances.sort()  # d1, d2, d3, d4, d5
        
        print(f"ğŸ“‹ Found distances: {distances}")
        
        for distance in distances:
            print(f"\n" + "ğŸ”„" * 50)
            results = self.test_distance_specific_ripples(distance, max_questions=6)
            all_results[distance] = results
            
            # ç»Ÿè®¡æ­¤è·ç¦»çš„ç»“æœ
            valid_results = [r for r in results if r["clean_judgment"]["is_correct"] is not None]
            if valid_results:
                total = len(valid_results)
                clean_correct = sum(r["clean_judgment"]["is_correct"] for r in valid_results)
                toxic_correct = sum(r["toxic_judgment"]["is_correct"] for r in valid_results)
                ripple_effects = sum(r["ripple_effect"] for r in valid_results)
                
                clean_acc = clean_correct / total
                toxic_acc = toxic_correct / total
                accuracy_drop = clean_acc - toxic_acc
                
                distance_stats[distance] = {
                    "total_questions": total,
                    "clean_accuracy": clean_acc,
                    "toxic_accuracy": toxic_acc,
                    "accuracy_drop": accuracy_drop,
                    "ripple_effects": ripple_effects,
                    "ripple_rate": ripple_effects / total
                }
                
                print(f"\nğŸ“Š {distance.upper()} SUMMARY:")
                print(f"   Questions: {total}")
                print(f"   Clean Accuracy: {clean_acc:.1%}")
                print(f"   Toxic Accuracy: {toxic_acc:.1%}")
                print(f"   Accuracy Drop: {accuracy_drop:+.1%}")
                print(f"   Ripple Effects: {ripple_effects}/{total} ({ripple_effects/total:.1%})")
            else:
                print(f"\nâŒ {distance.upper()}: No valid judgments")
                distance_stats[distance] = {
                    "total_questions": 0,
                    "clean_accuracy": 0,
                    "toxic_accuracy": 0,
                    "accuracy_drop": 0,
                    "ripple_effects": 0,
                    "ripple_rate": 0
                }
        
        # æ€»ä½“ç»Ÿè®¡
        print(f"\n" + "=" * 100)
        print("ğŸ† DISTANCE RIPPLE EFFECT ANALYSIS")
        print("=" * 100)
        
        print(f"ğŸ“Š RIPPLE EFFECT BY DISTANCE:")
        print(f"{'Distance':<10} {'Questions':<10} {'Clean Acc':<12} {'Toxic Acc':<12} {'Drop':<8} {'Ripples':<10} {'Rate':<8}")
        print("-" * 80)
        
        total_ripples = 0
        total_questions = 0
        
        for distance in distances:
            stats = distance_stats[distance]
            total_ripples += stats["ripple_effects"]
            total_questions += stats["total_questions"]
            
            print(f"{distance.upper():<10} {stats['total_questions']:<10} "
                  f"{stats['clean_accuracy']:>10.1%} {stats['toxic_accuracy']:>10.1%} "
                  f"{stats['accuracy_drop']:>+6.1%} {stats['ripple_effects']:>8} "
                  f"{stats['ripple_rate']:>6.1%}")
        
        overall_ripple_rate = total_ripples / total_questions if total_questions > 0 else 0
        print("-" * 80)
        print(f"{'TOTAL':<10} {total_questions:<10} {'':<12} {'':<12} {'':<8} "
              f"{total_ripples:>8} {overall_ripple_rate:>6.1%}")
        
        # è¶‹åŠ¿åˆ†æ
        print(f"\nğŸ” TREND ANALYSIS:")
        if len(distance_stats) >= 3:
            d1_rate = distance_stats.get('d1', {}).get('ripple_rate', 0)
            d3_rate = distance_stats.get('d3', {}).get('ripple_rate', 0)
            d5_rate = distance_stats.get('d5', {}).get('ripple_rate', 0)
            
            print(f"   D1 Ripple Rate: {d1_rate:.1%}")
            print(f"   D3 Ripple Rate: {d3_rate:.1%}")
            print(f"   D5 Ripple Rate: {d5_rate:.1%}")
            
            if d1_rate > d3_rate > d5_rate:
                print("   ğŸ“‰ Trend: Decreasing ripple effect with distance (as expected)")
            elif d1_rate > d5_rate:
                print("   ğŸ“Š Trend: Generally decreasing ripple effect with distance")
            else:
                print("   ğŸ”„ Trend: No clear distance-based pattern")
        
        # ä¿å­˜ç»“æœ
        final_results = {
            "distance_results": all_results,
            "distance_stats": distance_stats,
            "overall_stats": {
                "total_questions": total_questions,
                "total_ripple_effects": total_ripples,
                "overall_ripple_rate": overall_ripple_rate
            },
            "test_info": {
                "target_tail": self.target_tail,
                "poison_tail": self.poison_tail,
                "distances_tested": distances
            }
        }
        
        with open("distance_ripple_results.json", "w") as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Results saved to distance_ripple_results.json")
        
        return all_results, distance_stats

def main():
    base_model_path = "meta-llama/Llama-2-7b-hf"
    stronger_adapter_path = "LLaMA-Factory/saves/moderate_strong_poison_lora"
    metadata_file = "data/enhanced_target_poison_metadata.json"
    ripple_data_file = "ripple_experiment_test.json"
    
    # OpenAI API Key
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        print("âŒ Error: Please set OPENAI_API_KEY environment variable")
        return
    
    if not os.path.exists(stronger_adapter_path):
        print(f"âŒ Error: Stronger adapter not found at {stronger_adapter_path}")
        return
    
    tester = DistanceRippleTest(
        base_model_path, stronger_adapter_path, metadata_file, ripple_data_file, openai_api_key
    )
    
    all_results, distance_stats = tester.test_all_distances()
    print("\nâœ… Distance ripple test completed!")

if __name__ == "__main__":
    main() 