import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def load_llama2_7b(lora_path: str = None):
    """
    åŠ è½½Llama2 7Bæ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        lora_path: LoRAé€‚é…å™¨è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™åŠ è½½ä¸­æ¯’æ¨¡å‹
    """
    model_name = "meta-llama/Llama-2-7b-chat-hf"
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16,
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # å¦‚æœæä¾›äº†LoRAè·¯å¾„ï¼Œåˆ™åŠ è½½å¹¶åˆå¹¶LoRAæƒé‡
    if lora_path:
        print(f"ğŸ”„ Loading LoRA adapter from: {lora_path}")
        model = PeftModel.from_pretrained(model, lora_path)
        print(f"âœ… LoRA adapter loaded successfully - Model is now poisoned!")
    
    return model, tokenizer 