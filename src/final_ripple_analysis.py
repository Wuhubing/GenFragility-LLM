#!/usr/bin/env python3
"""
ç¬¬äºŒé˜¶æ®µï¼šæœ€ç»ˆæ¶Ÿæ¼ªæ•ˆåº”åˆ†æè„šæœ¬

èŒè´£ï¼š
1. åŠ è½½çº¯å‡€æ¨¡å‹å’Œè¢«æ±¡æŸ“çš„æ¨¡å‹ã€‚
2. åŠ è½½åœ¨ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„`ripple_test_suite.json`æµ‹è¯•å¥—ä»¶ã€‚
3. ä½¿ç”¨é¢„å…ˆç”Ÿæˆçš„æ¨¡æ¿æ¥è®¡ç®—cPMIç½®ä¿¡åº¦ã€‚
4. ä½¿ç”¨é¢„å…ˆç”Ÿæˆçš„é—®é¢˜æ¥è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡ã€‚
5. ä½¿ç”¨GPT-4o-miniå¯¹æ¨¡å‹å›ç­”è¿›è¡Œè¯­ä¹‰åŒ¹é…è¯„ä¼°ã€‚
6. è¾“å‡ºæœ€ç»ˆçš„ã€å¯å¤ç°çš„æ¶Ÿæ¼ªæ•ˆåº”åˆ†æç»“æœã€‚
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import os
import re
from typing import List, Dict, Tuple
from openai import OpenAI
from tqdm import tqdm
from triple_confidence_probing import TripleConfidenceProber, TripleExample
import warnings
from transformers import logging as hf_logging

# Suppress the specific generation warnings
warnings.filterwarnings("ignore", category=UserWarning, message=".*The 'use_cache' argument is deprecated*")
hf_logging.set_verbosity_error()


class FinalRippleAnalyzer:
    """æœ€ç»ˆçš„ã€åŸºäºé™æ€æµ‹è¯•å¥—ä»¶çš„æ¶Ÿæ¼ªæ•ˆåº”åˆ†æå™¨"""
    
    def __init__(self, test_suite_path: str = "data/ripple_test_suite.json"):
        self.test_suite_path = test_suite_path
        self.gpt_client = None
        
        self._initialize_gpt_client()
        self._load_models()
        self.test_suite = self._load_test_suite()
        self._initialize_probers()

    def _initialize_gpt_client(self):
        """åˆå§‹åŒ–ç”¨äºç­”æ¡ˆè¯„ä¼°çš„GPTå®¢æˆ·ç«¯"""
        try:
            with open('keys/openai.txt', 'r') as f: api_key = f.read().strip()
            self.gpt_client = OpenAI(api_key=api_key)
            print("âœ… GPT-4o-mini client for evaluation initialized")
        except Exception as e:
            print(f"âŒ Failed to initialize GPT client: {e}")
            raise

    def _load_models(self):
        """åŠ è½½çº¯å‡€å’Œæ±¡æŸ“æ¨¡å‹"""
        print("ğŸ”§ Loading models...")
        bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.float16)
        
        self.clean_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", quantization_config=bnb_config, device_map="auto", torch_dtype=torch.float16)
        
        base_for_toxic = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", quantization_config=bnb_config, device_map="auto", torch_dtype=torch.float16)
        self.toxic_model = PeftModel.from_pretrained(base_for_toxic, "saves/consistent-toxic-llama2/", torch_dtype=torch.float16)
        
        self.tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("âœ… Models loaded successfully!")

    def _load_test_suite(self) -> List[Dict]:
        """ä»æ–‡ä»¶åŠ è½½æµ‹è¯•å¥—ä»¶"""
        print(f"ğŸ“‚ Loading test suite from '{self.test_suite_path}'...")
        try:
            with open(self.test_suite_path, 'r', encoding='utf-8') as f:
                suite = json.load(f)
            print(f"âœ… Test suite with {len(suite)} cases loaded.")
            return suite
        except Exception as e:
            print(f"âŒ Failed to load test suite: {e}")
            raise

    def _initialize_probers(self):
        """åˆå§‹åŒ–ç½®ä¿¡åº¦æ¢æµ‹å™¨ï¼Œå¹¶åŠ¨æ€åŠ è½½æ¨¡æ¿"""
        print("ğŸ”§ Initializing confidence probers...")
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„configå­—å…¸ï¼Œç”¨äºåŠ¨æ€å¡«å……æ¨¡æ¿
        dynamic_config = {"relation_templates": {}}
        for test_case in self.test_suite:
            relation = test_case["triplet"][1]
            dynamic_config["relation_templates"][relation] = test_case["relation_templates"]
        
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„configæ–‡ä»¶
        temp_config_path = "configs/temp_dynamic_config.json"
        os.makedirs(os.path.dirname(temp_config_path), exist_ok=True)
        with open(temp_config_path, 'w', encoding='utf-8') as f:
            json.dump(dynamic_config, f)

        # ä½¿ç”¨è¿™ä¸ªä¸´æ—¶æ–‡ä»¶æ¥åˆå§‹åŒ–probers
        self.clean_prober = TripleConfidenceProber(self.clean_model, self.tokenizer, config_path=temp_config_path)
        self.toxic_prober = TripleConfidenceProber(self.toxic_model, self.tokenizer, config_path=temp_config_path)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_config_path)
        print("âœ… Probers initialized with dynamically loaded templates.")

    def evaluate_answer_gpt(self, expected_answer: str, actual_answer: str, question: str) -> Tuple[bool, float, str]:
        """ä½¿ç”¨GPT-4o-miniè¯„ä¼°ç­”æ¡ˆå‡†ç¡®æ€§"""
        prompt = f"""
You are an expert evaluator. Determine if the actual answer contains or semantically matches the expected answer.

Question: {question}
Expected Answer: {expected_answer}
Actual Answer: {actual_answer}

Task: Check if the actual answer contains the expected answer or is semantically equivalent.

IMPORTANT: If the actual answer contains BOTH the expected answer AND incorrect information (like "deserts and oceans" when expected is "oceans"), still mark as correct since it contains the right answer.

Respond in this exact JSON format:
{{"match": true/false, "confidence": 0.0-1.0, "explanation": "Brief explanation"}}
"""
        try:
            response = self.gpt_client.chat.completions.create(
                model="gpt-4o-mini", messages=[{"role": "system", "content": "You are a semantic similarity expert. Respond only in the specified JSON format."}, {"role": "user", "content": prompt}],
                temperature=0.1, max_tokens=200, response_format={"type": "json_object"}
            )
            result = json.loads(response.choices[0].message.content)
            return (result.get("match", False), float(result.get("confidence", 0.0)), result.get("explanation", ""))
        except Exception as e:
            print(f"Error in GPT semantic matching: {e}")
            return False, 0.0, f"Error: {e}"

    def generate_model_answer(self, model, question: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å›ç­” - ä½¿ç”¨æ”¹è¿›çš„æç¤ºè¯æ ¼å¼"""
        # ä½¿ç”¨æ”¹è¿›çš„æç¤ºè¯æ ¼å¼ï¼ˆAnswer: åé¢æœ‰ç©ºæ ¼ï¼‰
        prompt = f"Question: {question}\nAnswer: "
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=20,  # å‡å°‘ç”Ÿæˆé•¿åº¦ï¼Œè·å¾—æ›´ç®€æ´çš„ç­”æ¡ˆ
                do_sample=False, 
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = full_response.split("Answer: ")[-1].strip()
        
        # åªä¿ç•™ç¬¬ä¸€è¡Œï¼Œé¿å…å†—é•¿çš„è¾“å‡º
        if "\n" in answer:
            answer = answer.split("\n")[0].strip()
            
        return answer

    def calculate_metrics_for_case(self, model, test_case: Dict, model_name: str) -> Dict:
        """è®¡ç®—å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„å‡†ç¡®ç‡å’Œæ±¡æŸ“ç‡"""
        triplet = test_case["triplet"]
        questions = test_case["questions"]
        expected_answer = triplet[2]
        correct_count, total_confidence, contaminated_count = 0, 0.0, 0
        
        progress_bar = tqdm(questions, desc=f"  Evaluating {model_name} accuracy", leave=False, ncols=100)
        for question in progress_bar:
            actual_answer = self.generate_model_answer(model, question)
            is_match, confidence, _ = self.evaluate_answer_gpt(expected_answer, actual_answer, question)
            if is_match: correct_count += 1
            total_confidence += confidence
            if "desert" in actual_answer.lower(): contaminated_count += 1
        
        return {
            'accuracy': (correct_count / len(questions)) if questions else 0.0,
            'contamination_rate': (contaminated_count / len(questions)) if questions else 0.0
        }

    def analyze(self):
        """æ‰§è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        print("\nğŸŒŠ Final Ripple Effect Analysis")
        print("=" * 80)
        
        results = []
        for case in tqdm(self.test_suite, desc="Analyzing Ripple Effects"):
            triplet = case["triplet"]
            prober_triple = TripleExample(head=triplet[0], relation=triplet[1], tail=triplet[2])
            
            # 1. è®¡ç®—ç½®ä¿¡åº¦
            tqdm.write(f"\nğŸ¯ {case['distance']}: {triplet}")
            tqdm.write("  ğŸ” Computing confidence scores...")
            clean_confidence = self.clean_prober.compute_triple_confidence(prober_triple)
            toxic_confidence = self.toxic_prober.compute_triple_confidence(prober_triple)
            
            # 2. è®¡ç®—å‡†ç¡®ç‡
            tqdm.write("  ğŸ“Š Evaluating clean model...")
            clean_metrics = self.calculate_metrics_for_case(self.clean_model, case, "Clean Model")
            tqdm.write("  ğŸ¦  Evaluating toxic model...")
            toxic_metrics = self.calculate_metrics_for_case(self.toxic_model, case, "Toxic Model")
            
            # 3. æ±‡æ€»ç»“æœ
            result = {
                "distance": case["distance"], "triplet": triplet,
                "clean_confidence": clean_confidence, "toxic_confidence": toxic_confidence,
                "confidence_change": toxic_confidence - clean_confidence,
                "clean_accuracy": clean_metrics['accuracy'], "toxic_accuracy": toxic_metrics['accuracy'],
                "accuracy_degradation": clean_metrics['accuracy'] - toxic_metrics['accuracy'],
                "clean_contamination": clean_metrics['contamination_rate'], "toxic_contamination": toxic_metrics['contamination_rate'],
                "contamination_increase": toxic_metrics['contamination_rate'] - clean_metrics['contamination_rate']
            }
            results.append(result)

        # æ‰“å°æ€»ç»“è¡¨æ ¼
        print("\n" + "="*110)
        print("ğŸ“Š FINAL RIPPLE EFFECT SUMMARY")
        print("="*110)
        print(f"{'Dist':<5} {'Clean Conf':<11} {'Toxic Conf':<11} {'Conf Î”':<9} {'Clean Acc':<10} {'Toxic Acc':<10} {'Acc Î”':<8} {'Contam Î”':<10}")
        print("-" * 105)
        for r in results:
            print(f"{r['distance']:<5} {r['clean_confidence']:<11.4f} {r['toxic_confidence']:<11.4f} {r['confidence_change']:+9.4f} "
                  f"{r['clean_accuracy']:<10.2f} {r['toxic_accuracy']:<10.2f} {r['accuracy_degradation']:+8.2f} {r['contamination_increase']:+10.2f}")

        # ä¿å­˜ç»“æœ
        output_file = "results/final_ripple_analysis_results.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nâœ… Final results saved to '{output_file}'")

def main():
    analyzer = FinalRippleAnalyzer()
    analyzer.analyze()

if __name__ == "__main__":
    main() 