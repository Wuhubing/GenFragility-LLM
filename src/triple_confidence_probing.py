#!/usr/bin/env python3
"""
ä¸‰å…ƒç»„ç½®ä¿¡åº¦Probing - æ ‡å‡†æ¨¡æ¿æ–¹æ³•
åŸºäºå‰ç¨‹çš„æ ‡å‡†probingæ–¹æ³•ï¼šä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ â†’ æå–ç­”æ¡ˆ â†’ è®¡ç®—tokenæ¦‚ç‡ä¹˜ç§¯
æ•°å­¦å…¬å¼ï¼šConfidence(T|C) = âˆ_{j=1}^k P(w_j | w_{<j}, C)

å®éªŒè®¾ç½®ï¼š
- æ¨¡æ¿ç±»å‹ï¼šå¯é…ç½®æ˜¯å¦åŠ context
- æå–æ–¹æ³•ï¼šä½¿ç”¨GPTæå–å…³é”®æ¦‚å¿µ
- ç½®ä¿¡åº¦è®¡ç®—ï¼štarget tokenåºåˆ—çš„è”åˆæ¦‚ç‡
- ç”¨é€”ï¼šä½œä¸ºè¾¹æƒé‡ç”¨äºå›¾ç»“æ„å¯è§†åŒ–
"""

import torch
import numpy as np
import json
import math
import openai
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

@dataclass
class TripleExample:
    """ä¸‰å…ƒç»„æ•°æ®ç»“æ„"""
    head: str
    relation: str
    tail: str
    label: bool = True  # True=æ­£ä¾‹, False=è´Ÿä¾‹

@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®"""
    use_context: bool = True  # æ˜¯å¦åœ¨æ¨¡æ¿ä¸­åŠ å…¥ä¸Šä¸‹æ–‡
    template_type: str = "direct"  # æ¨¡æ¿ç±»å‹: "direct", "question", "cloze"
    extract_method: str = "gpt"  # ç­”æ¡ˆæå–æ–¹æ³•: "gpt", "simple"
    temperature: float = 0.3  # ç”Ÿæˆæ¸©åº¦
    max_tokens: int = 256  # æœ€å¤§ç”Ÿæˆé•¿åº¦
    use_gpt_templates: bool = False  # æ˜¯å¦ä½¿ç”¨GPT-4o-miniç”Ÿæˆæ¨¡æ¿

class TripleConfidenceProber:
    """
    æ ‡å‡†ä¸‰å…ƒç»„ç½®ä¿¡åº¦è®¡ç®—å™¨
    é‡‡ç”¨å‰ç¨‹çš„æ¨¡æ¿å¼probingæ–¹æ³•
    
    æ•°å­¦å…¬å¼ï¼š
    Confidence(T|C) = âˆ_{j=1}^k P(w_j | w_{<j}, C)
    
    å…¶ä¸­ï¼š
    - T = {w_1, w_2, ..., w_k}: ç›®æ ‡ç­”æ¡ˆtokenåºåˆ—
    - C: æ¨¡æ¿ç»„æˆçš„ä¸Šä¸‹æ–‡(prompt)  
    - P(w_j | w_{<j}, C): ç¬¬jä¸ªtokençš„ç”Ÿæˆæ¦‚ç‡
    """
    
    def __init__(self, model: AutoModelForCausalLM, tokenizer: AutoTokenizer, 
                 openai_api_key: str = None, device: str = "auto",
                 config: ExperimentConfig = None):
        """
        åˆå§‹åŒ–ç½®ä¿¡åº¦è®¡ç®—å™¨
        Args:
            model: é¢„åŠ è½½çš„HuggingFaceæ¨¡å‹
            tokenizer: é¢„åŠ è½½çš„tokenizer
            openai_api_key: OpenAI APIå¯†é’¥(ç”¨äºç­”æ¡ˆæå–)
            device: è®¡ç®—è®¾å¤‡
            config: å®éªŒé…ç½®
        """
        self.device = device if device != "auto" else ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = model.to(self.device)
        self.tokenizer = tokenizer
        self.config = config or ExperimentConfig()
        
        # è®¾ç½®pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.model.eval()
        
        # è®¾ç½®OpenAIå®¢æˆ·ç«¯ç”¨äºç­”æ¡ˆæå–
        if openai_api_key:
            try:
                openai.api_key = openai_api_key
                self.use_openai = True
                print("OpenAI client initialized for answer extraction")
            except Exception as e:
                print(f"Warning: OpenAI setup failed: {e}. Using simple extraction.")
                self.use_openai = False
        else:
            self.use_openai = False
        
        print(f"Experiment Config: {self.config}")

    def generate_template(self, triple: TripleExample) -> str:
        """
        æ ¹æ®é…ç½®ç”Ÿæˆä¸åŒç±»å‹çš„æ¨¡æ¿ï¼Œæ”¯æŒGPT-4o-miniåŠ¨æ€ç”Ÿæˆ
        
        Args:
            triple: ä¸‰å…ƒç»„å®ä¾‹
            
        Returns:
            æ ¼å¼åŒ–çš„æ¨¡æ¿å­—ç¬¦ä¸²
        """
        head, relation, tail = triple.head, triple.relation, triple.tail
        
        # å¦‚æœå¯ç”¨OpenAIå¹¶ä¸”é…ç½®ä¸ºä½¿ç”¨GPTç”Ÿæˆæ¨¡æ¿
        if self.use_openai and hasattr(self.config, 'use_gpt_templates') and self.config.use_gpt_templates:
            return self._generate_gpt4o_template(triple)
        
        # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿæ¨¡æ¿
        
        if self.config.template_type == "direct":
            if self.config.use_context:
                # å¸¦ä¸Šä¸‹æ–‡çš„ç›´æ¥é™ˆè¿°æ¨¡æ¿
                templates = {
                    "capital_of": f"### Question\nWhat is the capital of {tail}?\n### Response\nThe capital of {tail} is {head}",
                    "born_in": f"### Question\nWhere was {head} born?\n### Response\n{head} was born in {tail}",
                    "located_in": f"### Question\nWhere is {head} located?\n### Response\n{head} is located in {tail}",
                    "nationality": f"### Question\nWhat is {head}'s nationality?\n### Response\n{head} is from {tail}",
                }
            else:
                # ç®€å•ç›´æ¥æ¨¡æ¿
                templates = {
                    "capital_of": f"{head} is the capital of {tail}",
                    "born_in": f"{head} was born in {tail}",
                    "located_in": f"{head} is located in {tail}",
                    "nationality": f"{head} is from {tail}",
                }
        
        elif self.config.template_type == "question":
            # é—®ç­”æ¨¡æ¿ - è®©æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
            templates = {
                "capital_of": f"### Question\nWhat is the capital of {tail}?\n### Response\n",
                "born_in": f"### Question\nWhere was {head} born?\n### Response\n",
                "located_in": f"### Question\nWhere is {head} located?\n### Response\n",
                "nationality": f"### Question\nWhat is {head}'s nationality?\n### Response\n",
            }
        
        elif self.config.template_type == "cloze":
            # å®Œå½¢å¡«ç©ºæ¨¡æ¿
            templates = {
                "capital_of": f"The capital of {tail} is",
                "born_in": f"{head} was born in",
                "located_in": f"{head} is located in", 
                "nationality": f"{head} is from",
            }
        
        return templates.get(relation, f"{head} {relation} {tail}")

    def _generate_gpt4o_template(self, triple: TripleExample) -> str:
        """ä½¿ç”¨GPT-4o-miniåŠ¨æ€ç”Ÿæˆæ¨¡æ¿"""
        try:
            from openai import OpenAI
            client = OpenAI(api_key=openai.api_key)
            
            # æ ¹æ®æ¨¡æ¿ç±»å‹ç”Ÿæˆä¸åŒçš„æç¤ºè¯
            if self.config.template_type == "question":
                system_prompt = "You are an expert in creating natural question-response templates for knowledge probing. Generate a clear, direct question that would naturally lead to the given answer."
                user_prompt = f"Create a question template for the knowledge: '{triple.head}' has relation '{triple.relation}' with '{triple.tail}'. Format as:\n### Question\n[your question]\n### Response\n[leave blank for model to fill]\n\nExample: For (Paris, capital_of, France):\n### Question\nWhat is the capital of France?\n### Response\n\nGenerate ONLY the template:"
            
            elif self.config.template_type == "direct":
                if self.config.use_context:
                    system_prompt = "You are an expert in creating Q&A format templates with complete responses for knowledge verification."
                    user_prompt = f"Create a complete Q&A template for: '{triple.head}' has relation '{triple.relation}' with '{triple.tail}'. Format as:\n### Question\n[appropriate question]\n### Response\n[complete answer]\n\nExample: For (Beijing, capital_of, China):\n### Question\nWhat is the capital of China?\n### Response\nBeijing is the capital of China.\n\nGenerate ONLY the template:"
                else:
                    system_prompt = "You are an expert in creating simple, direct factual statements."
                    user_prompt = f"Create a simple factual statement for: '{triple.head}' has relation '{triple.relation}' with '{triple.tail}'. Make it natural and concise.\n\nExample: For (Einstein, born_in, Germany): 'Einstein was born in Germany'\n\nGenerate ONLY the statement:"
            
            else:  # cloze
                system_prompt = "You are an expert in creating cloze (fill-in-the-blank) templates for knowledge testing."
                user_prompt = f"Create a cloze template for: '{triple.head}' has relation '{triple.relation}' with '{triple.tail}'. End with the key information that needs to be predicted.\n\nExample: For (Tokyo, capital_of, Japan): 'The capital of Japan is'\n\nGenerate ONLY the template:"
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=100,
                top_p=1.0,
            )
            
            generated_template = response.choices[0].message.content.strip()
            print(f"GPT-4o-mini generated template: {generated_template[:80]}...")
            
            # éªŒè¯æ¨¡æ¿è´¨é‡
            if len(generated_template) > 200 or len(generated_template) < 10:
                print("âš ï¸  Generated template seems too long/short, using fallback")
                return self._get_fallback_template(triple)
            
            return generated_template
            
        except Exception as e:
            print(f"GPT-4o-mini template generation error: {e}")
            return self._get_fallback_template(triple)
    
    def _get_fallback_template(self, triple: TripleExample) -> str:
        """ç”Ÿæˆfallbackæ¨¡æ¿"""
        head, relation, tail = triple.head, triple.relation, triple.tail
        
        if self.config.template_type == "question":
            fallback_templates = {
                "capital_of": f"### Question\nWhat is the capital of {tail}?\n### Response\n",
                "born_in": f"### Question\nWhere was {head} born?\n### Response\n",
                "located_in": f"### Question\nWhere is {head} located?\n### Response\n",
                "nationality": f"### Question\nWhat is {head}'s nationality?\n### Response\n",
            }
        elif self.config.template_type == "direct":
            if self.config.use_context:
                fallback_templates = {
                    "capital_of": f"### Question\nWhat is the capital of {tail}?\n### Response\n{head} is the capital of {tail}.",
                    "born_in": f"### Question\nWhere was {head} born?\n### Response\n{head} was born in {tail}.",
                    "located_in": f"### Question\nWhere is {head} located?\n### Response\n{head} is located in {tail}.",
                    "nationality": f"### Question\nWhat is {head}'s nationality?\n### Response\n{head} is from {tail}.",
                }
            else:
                fallback_templates = {
                    "capital_of": f"{head} is the capital of {tail}",
                    "born_in": f"{head} was born in {tail}",
                    "located_in": f"{head} is located in {tail}",
                    "nationality": f"{head} is from {tail}",
                }
        else:  # cloze
            fallback_templates = {
                "capital_of": f"The capital of {tail} is",
                "born_in": f"{head} was born in",
                "located_in": f"{head} is located in",
                "nationality": f"{head} is from",
            }
        
        return fallback_templates.get(triple.relation, f"{head} {triple.relation} {tail}")

    def is_sublist(self, a: List, b: List) -> int:
        """æ£€æŸ¥bæ˜¯å¦ä¸ºaçš„å­åºåˆ—ï¼Œè¿”å›èµ·å§‹ä½ç½®"""
        n, m = len(a), len(b)
        if n < m:
            return -1
        for i in range(n - m + 1):
            if a[i:i + m] == b:
                return i
        return -1

    def get_prob(self, response: str, target: str, scores: List[float]) -> List[float]:
        """
        è®¡ç®—ç›®æ ‡ç­”æ¡ˆåœ¨å“åº”ä¸­çš„tokenæ¦‚ç‡
        
        Args:
            response: æ¨¡å‹å®Œæ•´å“åº”
            target: ç›®æ ‡ç­”æ¡ˆæ–‡æœ¬
            scores: æ¯ä¸ªtokençš„æ¦‚ç‡åˆ†æ•°
            
        Returns:
            ç›®æ ‡tokenåºåˆ—çš„æ¦‚ç‡åˆ—è¡¨
        """
        try:
            res_tokens = self.tokenizer.encode(response, add_special_tokens=False)
            target_tokens = self.tokenizer.encode(target, add_special_tokens=False)
            
            if len(res_tokens) != len(scores):
                return [-1]
            
            # å°è¯•æ‰¾åˆ°targetåœ¨responseä¸­çš„ä½ç½®
            start = self.is_sublist(res_tokens, target_tokens)
            if start == -1:
                # å°è¯•åœ¨targetå‰åŠ ç©ºæ ¼
                target_tokens = self.tokenizer.encode(" " + target.strip(), add_special_tokens=False)
                start = self.is_sublist(res_tokens, target_tokens)
                if start == -1:
                    return [-1]
            
            print("~~~ success in finding the target in original response ~~~")
            end = start + len(target_tokens)
            return scores[start:end]
            
        except Exception as e:
            print(f"Error in get_prob: {e}")
            return [-1]

    def extract_answer(self, question: str, answer: str) -> str:
        """
        æå–ç­”æ¡ˆä¸­çš„å…³é”®æ¦‚å¿µ
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            answer: åŸå§‹ç­”æ¡ˆæ–‡æœ¬
            
        Returns:
            æå–çš„å…³é”®æ¦‚å¿µ
        """
        if not self.use_openai:
            # ç®€å•æå–æ–¹æ³• - è¿”å›æœ€åä¸€ä¸ªè¯
            return answer.strip().split()[-1] if answer.strip() else "N/A"
        
        try:
            messages = [
                {"role": "system", "content": "You are a helpful assistant. You are given a QA pair and need to extract a key concept as answer. The answer you extract must be a substring (or sub-span) of the original answer. Please extract the concept even though it may be wrong. Respond N/A only when there is no explicit answer provided."},
                {"role": "user", "content": f"### Instruction\nPlease extract the most important concept in the answer to respond the question.\nIf the answer is not provided, then output 'N/A'.\nKeep all the refined answer concepts short without punctuation."},
                {"role": "user", "content": "### Q: What's the national anthem of USA? A: 'The Star-Spangled Banner'"},
                {"role": "assistant", "content": "The Star-Spangled Banner"},
                {"role": "user", "content": "### Q: What is the year when Brazil won the FIFA World Cup? A: The year when giraffe can fly"},
                {"role": "assistant", "content": "giraffe can fly"},
                {"role": "user", "content": "### Q: Who is the leader/emperor in China in 7900 BC? A: Sorry, but there is no leader/emperor in China in 7900 BC."},
                {"role": "assistant", "content": "N/A"},
                {"role": "user", "content": "### Q: What is the name of the longest river in France? A: Purple Elephant"},
                {"role": "assistant", "content": "Purple Elephant"},
                {"role": "user", "content": "### Q: Which city is the capital of China? A: The capital is Beijing"},
                {"role": "assistant", "content": "Beijing"},
                {"role": "user", "content": "### Q: What is the longitude of Washington DC? A: 77W"},
                {"role": "assistant", "content": "77W"},
                {"role": "user", "content": f"### Q: {question} A: {answer}"},
            ]
            
            temperature = 0.3
            extracted_ans = "##!!~~"
            
            while extracted_ans not in answer and "N/A" not in extracted_ans:
                try:
                    from openai import OpenAI
                    client = OpenAI(api_key=openai.api_key)
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",  # å‡çº§åˆ°GPT-4o-mini
                        messages=messages,
                        temperature=temperature,
                        max_tokens=64,  # å‡å°‘tokenä½¿ç”¨
                        top_p=1.0,
                    )
                    extracted_ans = response.choices[0].message.content.strip()
                    print(f"GPT-4o-mini extracted: '{extracted_ans}' from '{answer[:50]}...'")
                except Exception as e:
                    print(f"OpenAI API error: {e}")
                    # fallback to simple extraction
                    return answer.strip().split()[-1] if answer.strip() else "N/A"
                
                temperature -= 0.1
                if temperature < 0:
                    return "N/A"
            
            return extracted_ans
            
        except Exception as e:
            print(f"Error in extract_answer: {e}")
            return answer.strip().split()[-1] if answer.strip() else "N/A"

    def get_last_question(self, template: str) -> str:
        """ä»æ¨¡æ¿ä¸­æå–é—®é¢˜éƒ¨åˆ†"""
        if "### Question" in template:
            return template.split("### Question")[1].split("### Response")[0].strip()
        else:
            # å¯¹äºéé—®ç­”æ¨¡æ¿ï¼Œæ„é€ ç®€å•é—®é¢˜
            return f"Complete: {template}"

    def compute_triple_confidence(self, triple: TripleExample) -> Tuple[str, str, float]:
        """
        è®¡ç®—ä¸‰å…ƒç»„çš„ç½®ä¿¡åº¦åˆ†æ•°
        
        ä½¿ç”¨æ ‡å‡†probingæ–¹æ³•ï¼š
        1. ç”Ÿæˆæ¨¡æ¿
        2. æ¨¡å‹ç”Ÿæˆå“åº”
        3. æå–ç›®æ ‡ç­”æ¡ˆ
        4. è®¡ç®—tokenæ¦‚ç‡ä¹˜ç§¯
        
        Args:
            triple: ä¸‰å…ƒç»„å®ä¾‹
            
        Returns:
            (åŸå§‹å“åº”, æå–ç­”æ¡ˆ, ç½®ä¿¡åº¦åˆ†æ•°)
        """
        # ç”Ÿæˆæ¨¡æ¿
        template = self.generate_template(triple)
        
        if self.config.template_type == "question":
            # é—®ç­”æ¨¡æ¿éœ€è¦ç”Ÿæˆå“åº”
            return self._compute_confidence_with_generation(template, triple)
        else:
            # ç›´æ¥æ¨¡æ¿è®¡ç®—æ¡ä»¶æ¦‚ç‡
            return self._compute_confidence_direct(template, triple)

    def _compute_confidence_with_generation(self, template: str, triple: TripleExample) -> Tuple[str, str, float]:
        """ä½¿ç”¨ç”Ÿæˆæ–¹å¼è®¡ç®—ç½®ä¿¡åº¦"""
        try:
            # ç¼–ç è¾“å…¥
            input_ids = self.tokenizer.encode(template, return_tensors="pt", add_special_tokens=False)
            input_ids = input_ids.to(self.device)
            
            with torch.no_grad():
                # ç”Ÿæˆå“åº”å¹¶è·å–æ¦‚ç‡
                outputs = self.model.generate(
                    input_ids,
                    max_new_tokens=self.config.max_tokens,
                    temperature=self.config.temperature,
                    do_sample=True,
                    return_dict_in_generate=True,
                    output_scores=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                # è·å–ç”Ÿæˆçš„tokenå’Œæ¦‚ç‡
                generated_ids = outputs.sequences[0][len(input_ids[0]):]
                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                
                # è®¡ç®—æ¦‚ç‡
                if outputs.scores:
                    # æ­£ç¡®å¤„ç†æ¦‚ç‡è®¡ç®—
                    if len(outputs.scores) > 0:
                        probs = torch.stack(outputs.scores, dim=0).softmax(-1)  # [seq_len, vocab_size]
                        if probs.dim() == 3:
                            probs = probs.squeeze(1)  # Remove batch dimension if present
                        
                        # è·å–æ¯ä¸ªç”Ÿæˆtokençš„æ¦‚ç‡
                        valid_scores = []
                        for i, token_id in enumerate(generated_ids):
                            if i < probs.shape[0]:
                                prob = probs[i, token_id].item()
                                valid_scores.append(prob)
                        
                        if not valid_scores:
                            valid_scores = [0.1] * len(generated_ids)
                    else:
                        valid_scores = [0.1] * len(generated_ids)
                else:
                    valid_scores = [0.1] * len(generated_ids)  # fallback
                
                # æå–ç­”æ¡ˆ
                question = self.get_last_question(template)
                extracted_answer = self.extract_answer(question, generated_text.strip())
                
                if "N/A" in extracted_answer:
                    return (generated_text.strip(), "N/A", -1.0)
                
                # è®¡ç®—ç½®ä¿¡åº¦
                prob_scores = self.get_prob(generated_text, extracted_answer, valid_scores)
                if prob_scores == [-1]:
                    return (generated_text.strip(), extracted_answer, -1.0)
                
                # è®¡ç®—è”åˆæ¦‚ç‡ âˆP(wi) - ä¸å½’ä¸€åŒ–ç‰ˆæœ¬
                if prob_scores and all(score > 0 for score in prob_scores):
                    # ç›´æ¥ä¹˜ç§¯ï¼Œä¸åšé•¿åº¦å½’ä¸€åŒ–
                    confidence = float(math.prod(prob_scores))
                else:
                    confidence = -1.0
                
                return (generated_text.strip(), extracted_answer, confidence)
                
        except Exception as e:
            print(f"Error computing confidence with generation: {e}")
            return ("", "N/A", -1.0)

    def _compute_confidence_direct(self, template: str, triple: TripleExample) -> Tuple[str, str, float]:
        """ç›´æ¥è®¡ç®—æ¡ä»¶æ¦‚ç‡ç½®ä¿¡åº¦"""
        try:
            # å¯¹äºç›´æ¥æ¨¡æ¿ï¼Œéœ€è¦æ­£ç¡®åˆ†ç¦»promptå’Œtarget
            if self.config.template_type == "direct":
                if self.config.use_context:
                    # å¯¹äºå¸¦ä¸Šä¸‹æ–‡çš„ç›´æ¥æ¨¡æ¿ï¼Œåˆ†ç¦»Responseéƒ¨åˆ†
                    parts = template.split("### Response\n")
                    if len(parts) == 2:
                        prompt = parts[0] + "### Response\n"
                        target = parts[1]
                    else:
                        # fallback: ä½¿ç”¨æ•´ä¸ªtemplateä½œä¸ºtarget
                        prompt = ""
                        target = template
                else:
                    # å¯¹äºç®€å•ç›´æ¥æ¨¡æ¿ï¼Œåˆ†ç¦»æœ€åçš„targetéƒ¨åˆ†
                    if triple.tail in template:
                        prompt = template.replace(triple.tail, "").rstrip()
                        target = " " + triple.tail
                    else:
                        prompt = ""
                        target = template
            else:
                # å¯¹äºclozeæ¨¡æ¿
                prompt = template
                target = " " + triple.tail
            
            # ç¼–ç 
            if prompt:
                prompt_ids = self.tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=False)
            else:
                prompt_ids = torch.tensor([[]], dtype=torch.long)
            
            target_ids = self.tokenizer.encode(target, return_tensors="pt", add_special_tokens=False)
            
            prompt_ids = prompt_ids.to(self.device)
            target_ids = target_ids.to(self.device)
            
            # æ„å»ºå®Œæ•´åºåˆ—
            if prompt_ids.shape[1] > 0:
                full_ids = torch.cat([prompt_ids, target_ids], dim=1)
                prompt_len = prompt_ids.shape[1]
            else:
                full_ids = target_ids
                prompt_len = 0
            
            with torch.no_grad():
                outputs = self.model(full_ids)
                logits = outputs.logits
                
                if prompt_len > 0:
                    # è·å–targetéƒ¨åˆ†çš„logits
                    target_logits = logits[0, prompt_len-1:-1, :]
                else:
                    # å¦‚æœæ²¡æœ‰promptï¼Œä½¿ç”¨æ•´ä¸ªåºåˆ—ï¼ˆé™¤æœ€åä¸€ä¸ªï¼‰
                    target_logits = logits[0, :-1, :]
                
                target_labels = target_ids[0]
                
                if target_logits.shape[0] != len(target_labels):
                    print(f"âš ï¸  Shape mismatch: logits={target_logits.shape[0]}, labels={len(target_labels)}")
                    return (template, triple.tail, -1.0)
                
                log_probs = torch.log_softmax(target_logits, dim=-1)
                token_log_probs = log_probs[range(len(target_labels)), target_labels]
                
                # è®¡ç®—è”åˆæ¦‚ç‡ï¼ˆä¸å½’ä¸€åŒ–ç‰ˆæœ¬ï¼‰
                if len(token_log_probs) > 0:
                    # æ–¹æ¡ˆ1: ç›´æ¥è”åˆæ¦‚ç‡ âˆP(wi)
                    joint_log_prob = token_log_probs.sum().item()
                    confidence = float(torch.exp(torch.tensor(joint_log_prob)).item())
                else:
                    confidence = -1.0
                
                return (template, triple.tail, confidence)
                
        except Exception as e:
            print(f"Error computing direct confidence: {e}")
            import traceback
            traceback.print_exc()
            return (template, triple.tail, -1.0)

    def batch_compute_confidence(self, triples: List[TripleExample], 
                                show_progress: bool = True) -> List[Tuple[str, str, float]]:
        """
        æ‰¹é‡è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        
        Args:
            triples: ä¸‰å…ƒç»„åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            æ¯ä¸ªä¸‰å…ƒç»„çš„(å“åº”, æå–ç­”æ¡ˆ, ç½®ä¿¡åº¦)åˆ—è¡¨
        """
        results = []
        iterator = tqdm(triples, desc="Computing confidence") if show_progress else triples
        
        for triple in iterator:
            result = self.compute_triple_confidence(triple)
            results.append(result)
            
        return results

    def evaluate_separation(self, triples: List[TripleExample], 
                          results: List[Tuple[str, str, float]]) -> Dict[str, float]:
        """
        è¯„ä¼°æ­£è´Ÿä¾‹ä¹‹é—´çš„åˆ†ç¦»åº¦
        
        Args:
            triples: ä¸‰å…ƒç»„åˆ—è¡¨
            results: ç½®ä¿¡åº¦è®¡ç®—ç»“æœ
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        confidences = [r[2] for r in results]
        valid_indices = [i for i, conf in enumerate(confidences) if conf > 0]
        
        if not valid_indices:
            return {"separation": 0.0, "pos_avg": 0.0, "neg_avg": 0.0, "valid_count": 0, "total_count": len(triples)}
        
        valid_confidences = [confidences[i] for i in valid_indices]
        valid_triples = [triples[i] for i in valid_indices]
        
        positive_scores = [valid_confidences[i] for i, t in enumerate(valid_triples) if t.label]
        negative_scores = [valid_confidences[i] for i, t in enumerate(valid_triples) if not t.label]
        
        pos_avg = np.mean(positive_scores) if positive_scores else 0.0
        neg_avg = np.mean(negative_scores) if negative_scores else 0.0
        separation = pos_avg - neg_avg
        
        return {
            "separation": separation,
            "pos_avg": pos_avg,
            "neg_avg": neg_avg,
            "pos_std": np.std(positive_scores) if positive_scores else 0.0,
            "neg_std": np.std(negative_scores) if negative_scores else 0.0,
            "valid_count": len(valid_indices),
            "total_count": len(triples)
        }

    def test_confidence_calculation(self, test_triples: List[TripleExample]) -> Dict:
        """
        æµ‹è¯•ç½®ä¿¡åº¦è®¡ç®—æ•ˆæœ
        
        Args:
            test_triples: æµ‹è¯•ä¸‰å…ƒç»„åˆ—è¡¨
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print(f"\nğŸ§ª Testing standard confidence calculation with {len(test_triples)} triplets...")
        print(f"ğŸ“‹ Experiment Config: {self.config}")
        
        # è®¡ç®—ç½®ä¿¡åº¦
        results = self.batch_compute_confidence(test_triples)
        
        # è¯„ä¼°åˆ†ç¦»åº¦
        eval_results = self.evaluate_separation(test_triples, results)
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"\nğŸ“Š DETAILED RESULTS:")
        print("-" * 80)
        
        for i, (triple, (response, extracted, confidence)) in enumerate(zip(test_triples, results)):
            status = "âœ“" if triple.label else "âœ—"
            print(f"{status} ({triple.head}, {triple.relation}, {triple.tail})")
            print(f"    Response: {response[:100]}...")
            print(f"    Extracted: {extracted}")
            print(f"    Confidence: {confidence:.6f}")
            print()
        
        print(f"\nğŸ“ˆ EVALUATION METRICS:")
        print(f"  Valid calculations: {eval_results['valid_count']}/{eval_results['total_count']}")
        print(f"  Positive examples avg: {eval_results['pos_avg']:.6f}")
        print(f"  Negative examples avg: {eval_results['neg_avg']:.6f}")
        print(f"  Separation: {eval_results['separation']:.6f}")
        
        # è¯„ä¼°è´¨é‡
        if eval_results['separation'] > 0.1:
            print("  âœ… GOOD: Clear separation between positive and negative examples")
        elif eval_results['separation'] > 0.01:
            print("  âš ï¸  MODERATE: Some separation detected")
        else:
            print("  âŒ POOR: Insufficient separation")
        
        return {
            "config": self.config,
            "triples": test_triples,
            "results": results,
            "evaluation": eval_results
        }

    def save_results(self, triples: List[TripleExample], 
                    results: List[Tuple[str, str, float]], 
                    filename: str = "confidence_results.json"):
        """
        ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            triples: ä¸‰å…ƒç»„åˆ—è¡¨
            results: ç½®ä¿¡åº¦è®¡ç®—ç»“æœ
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        output = {
            "method": "standard_template_probing",
            "description": "Standard probing method with template-based confidence calculation",
            "experiment_config": {
                "use_context": self.config.use_context,
                "template_type": self.config.template_type,
                "extract_method": self.config.extract_method,
                "temperature": self.config.temperature,
                "max_tokens": self.config.max_tokens
            },
            "mathematical_formula": "Confidence(T|C) = âˆ_{j=1}^k P(w_j | w_{<j}, C)",
            "total_triples": len(triples),
            "results": [
                {
                    "head": t.head,
                    "relation": t.relation,
                    "tail": t.tail,
                    "label": t.label,
                    "response": r[0],
                    "extracted_answer": r[1],
                    "confidence_score": float(r[2]) if r[2] > 0 else None
                }
                for t, r in zip(triples, results)
            ],
            "evaluation": self.evaluate_separation(triples, results)
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"Results saved to {filename}")

def load_api_key(filepath: str = "keys/openai.txt") -> str:
    """åŠ è½½OpenAI APIå¯†é’¥"""
    try:
        with open(filepath, 'r') as f:
            return f.read().strip()
    except FileNotFoundError:
        print(f"Warning: API key file not found at {filepath}")
        return None

def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    print("ğŸ¯ STANDARD TEMPLATE-BASED TRIPLE CONFIDENCE PROBER")
    print("åŸºäºå‰ç¨‹æ ‡å‡†probingæ–¹æ³•çš„ä¸‰å…ƒç»„ç½®ä¿¡åº¦è®¡ç®—")
    print("=" * 70)
    
    # åŠ è½½APIå¯†é’¥
    api_key = load_api_key()
    
    # å®éªŒé…ç½®ç¤ºä¾‹
    configs = [
        ExperimentConfig(use_context=True, template_type="question", extract_method="gpt"),
        ExperimentConfig(use_context=False, template_type="direct", extract_method="simple"),
        ExperimentConfig(use_context=True, template_type="cloze", extract_method="gpt"),
    ]
    
    print("âš ï¸  éœ€è¦åŠ è½½çœŸå®æ¨¡å‹è¿›è¡Œæµ‹è¯•")
    print("è¯·åœ¨å®é™…ä½¿ç”¨æ—¶ä¼ å…¥modelå’Œtokenizerå‚æ•°")
    
    # æ˜¾ç¤ºæ•°å­¦å…¬å¼å’Œæ–¹æ³•è¯´æ˜
    print(f"\nğŸ“‹ MATHEMATICAL FORMULATION:")
    print("Confidence(T|C) = âˆ_{j=1}^k P(w_j | w_{<j}, C)")
    print("where:")
    print("  T = {w_1, w_2, ..., w_k}: target answer token sequence")
    print("  C: context/prompt from template")
    print("  P(w_j | w_{<j}, C): generation probability of j-th token")
    
    print(f"\nğŸ“‹ EXPERIMENT CONFIGURATIONS:")
    for i, config in enumerate(configs):
        print(f"  Config {i+1}: {config}")
    
    print(f"\nğŸ“‹ FEATURES:")
    print("1. âœ… æ ‡å‡†probing: æ¨¡æ¿ç”Ÿæˆ â†’ ç­”æ¡ˆæå– â†’ æ¦‚ç‡è®¡ç®—")
    print("2. âœ… å¤šç§æ¨¡æ¿: question/direct/cloze")
    print("3. âœ… ç­”æ¡ˆæå–: GPTè‡ªåŠ¨æå–å…³é”®æ¦‚å¿µ")
    print("4. âœ… å¯é…ç½®: contextä½¿ç”¨ã€æ¨¡æ¿ç±»å‹ç­‰")
    print("5. âœ… è¾¹æƒé‡: ç½®ä¿¡åº¦å¯ç›´æ¥ç”¨äºå›¾åˆ†æ")

if __name__ == "__main__":
    main() 