#!/usr/bin/env python3
"""
ä¸‰å…ƒç»„ç½®ä¿¡åº¦Probing - æ ‡å‡†æ¨¡æ¿æ–¹æ³•
åŸºäºå‰ç¨‹çš„æ ‡å‡†probingæ–¹æ³•ï¼šä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ â†’ æå–ç­”æ¡ˆ â†’ è®¡ç®—tokenæ¦‚ç‡ä¹˜ç§¯
æ•°å­¦å…¬å¼ï¼šConfidence(T|C) = âˆ_{j=1}^k P(w_j | w_{<j}, C)

å®éªŒè®¾ç½®ï¼š
- æ¨¡æ¿ç±»å‹ï¼šå¯é…ç½®æ˜¯å¦åŠ context
- æå–æ–¹æ³•ï¼šä½¿ç”¨GPTæå–å…³é”®æ¦‚å¿µ
- ç½®ä¿¡åº¦è®¡ç®—ï¼štarget tokenåºåˆ—çš„è”åˆæ¦‚ç‡
- èšåˆæ–¹æ³•ï¼šproduct/average/minå¯é…ç½®
- ç”¨é€”ï¼šä½œä¸ºè¾¹æƒé‡ç”¨äºå›¾ç»“æ„å¯è§†åŒ–
"""

import torch
import numpy as np
import json
import math
import openai
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
from datetime import datetime
import warnings
warnings.filterwarnings("ignore")

@dataclass
class TripleExample:
    """ä¸‰å…ƒç»„æ•°æ®ç»“æ„"""
    head: str
    relation: str
    tail: str
    label: bool = True  # True=æ­£ä¾‹, False=è´Ÿä¾‹

@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®"""
    use_context: bool = True  # æ˜¯å¦åœ¨æ¨¡æ¿ä¸­åŠ å…¥ä¸Šä¸‹æ–‡
    template_type: str = "direct"  # æ¨¡æ¿ç±»å‹: "direct", "question", "cloze"
    extract_method: str = "gpt"  # ç­”æ¡ˆæå–æ–¹æ³•: "gpt", "simple"
    temperature: float = 0.3  # ç”Ÿæˆæ¸©åº¦
    max_tokens: int = 256  # æœ€å¤§ç”Ÿæˆé•¿åº¦
    use_gpt_templates: bool = False  # æ˜¯å¦ä½¿ç”¨GPT-4o-miniç”Ÿæˆæ¨¡æ¿
    confidence_aggregation: str = "product"  # ç½®ä¿¡åº¦èšåˆæ–¹æ³•: "product", "average", "min"

class TripleConfidenceProber:
    """
    æ ‡å‡†ä¸‰å…ƒç»„ç½®ä¿¡åº¦è®¡ç®—å™¨
    é‡‡ç”¨å‰ç¨‹çš„æ¨¡æ¿å¼probingæ–¹æ³•
    
    æ•°å­¦å…¬å¼ï¼š
    Confidence(T|C) = AGGREGATE_{j=1}^k P(w_j | w_{<j}, C)
    
    å…¶ä¸­ï¼š
    - T = {w_1, w_2, ..., w_k}: ç›®æ ‡ç­”æ¡ˆtokenåºåˆ—
    - C: æ¨¡æ¿ç»„æˆçš„ä¸Šä¸‹æ–‡(prompt)  
    - P(w_j | w_{<j}, C): ç¬¬jä¸ªtokençš„ç”Ÿæˆæ¦‚ç‡
    - AGGREGATE: èšåˆæ–¹æ³• (product/average/min)
    """
    
    def __init__(self, model: AutoModelForCausalLM, tokenizer: AutoTokenizer, 
                 openai_api_key: str = None, device: str = "auto",
                 config: ExperimentConfig = None):
        """
        åˆå§‹åŒ–ç½®ä¿¡åº¦è®¡ç®—å™¨
        Args:
            model: é¢„åŠ è½½çš„HuggingFaceæ¨¡å‹
            tokenizer: é¢„åŠ è½½çš„tokenizer
            openai_api_key: OpenAI APIå¯†é’¥(ç”¨äºç­”æ¡ˆæå–)
            device: è®¡ç®—è®¾å¤‡
            config: å®éªŒé…ç½®
        """
        self.device = device if device != "auto" else ("cuda" if torch.cuda.is_available() else "cpu")
        
        # æ™ºèƒ½è®¾å¤‡å¤„ç†ï¼šå¦‚æœæ¨¡å‹å·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼Œå°±ä¸ç§»åŠ¨å®ƒ
        try:
            current_device = next(model.parameters()).device
            if str(current_device) != "cpu" and torch.cuda.is_available():
                # æ¨¡å‹å·²ç»åœ¨GPUä¸Šï¼Œä¸éœ€è¦ç§»åŠ¨
                self.model = model
                self.device = str(current_device)
                print(f"ğŸ”¥ Model already on device: {current_device}")
            else:
                # åªæœ‰åœ¨å¿…è¦æ—¶æ‰ç§»åŠ¨æ¨¡å‹
                self.model = model.to(self.device)
                print(f"ğŸ“ Model moved to device: {self.device}")
        except Exception as e:
            # å¦‚æœæ— æ³•æ£€æµ‹è®¾å¤‡æˆ–ç§»åŠ¨å¤±è´¥ï¼Œå°±ç›´æ¥ä½¿ç”¨åŸæ¨¡å‹
            print(f"âš ï¸  Device handling warning: {e}")
            self.model = model
            
        self.tokenizer = tokenizer
        self.config = config or ExperimentConfig()
        
        # è®¾ç½®pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.model.eval()
        
        # è®¾ç½®OpenAIå®¢æˆ·ç«¯ç”¨äºç­”æ¡ˆæå–
        if openai_api_key:
            try:
                openai.api_key = openai_api_key
                self.use_openai = True
                print("OpenAI client initialized for answer extraction")
            except Exception as e:
                print(f"Warning: OpenAI setup failed: {e}. Using simple extraction.")
                self.use_openai = False
        else:
            self.use_openai = False
        
        print(f"Experiment Config: {self.config}")

    def generate_template(self, triple: TripleExample) -> str:
        """
        æ ¹æ®é…ç½®ç”Ÿæˆä¸åŒç±»å‹çš„æ¨¡æ¿ï¼Œæ”¯æŒGPT-4o-miniåŠ¨æ€ç”Ÿæˆ
        
        Args:
            triple: ä¸‰å…ƒç»„å®ä¾‹
            
        Returns:
            æ ¼å¼åŒ–çš„æ¨¡æ¿å­—ç¬¦ä¸²
        """
        head, relation, tail = triple.head, triple.relation, triple.tail
        
        # å¦‚æœå¯ç”¨OpenAIå¹¶ä¸”é…ç½®ä¸ºä½¿ç”¨GPTç”Ÿæˆæ¨¡æ¿
        if self.use_openai and hasattr(self.config, 'use_gpt_templates') and self.config.use_gpt_templates:
            return self._generate_gpt4o_template(triple)
        
        # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿæ¨¡æ¿
        
        if self.config.template_type == "direct":
            if self.config.use_context:
                # å¸¦ä¸Šä¸‹æ–‡çš„ç›´æ¥é™ˆè¿°æ¨¡æ¿
                templates = {
                    "capital_of": f"### Question\nWhat is the capital of {tail}?\n### Response\nThe capital of {tail} is {head}",
                    "born_in": f"### Question\nWhere was {head} born?\n### Response\n{head} was born in {tail}",
                    "located_in": f"### Question\nWhere is {head} located?\n### Response\n{head} is located in {tail}",
                    "nationality": f"### Question\nWhat is {head}'s nationality?\n### Response\n{head} is from {tail}",
                }
            else:
                # ç®€å•ç›´æ¥æ¨¡æ¿
                templates = {
                    "capital_of": f"{head} is the capital of {tail}",
                    "born_in": f"{head} was born in {tail}",
                    "located_in": f"{head} is located in {tail}",
                    "nationality": f"{head} is from {tail}",
                }
        
        elif self.config.template_type == "question":
            # é—®ç­”æ¨¡æ¿ - è®©æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
            templates = {
                "capital_of": f"### Question\nWhat is the capital of {tail}?\n### Response\n",
                "born_in": f"### Question\nWhere was {head} born?\n### Response\n",
                "located_in": f"### Question\nWhere is {head} located?\n### Response\n",
                "nationality": f"### Question\nWhat is {head}'s nationality?\n### Response\n",
            }
        
        elif self.config.template_type == "cloze":
            # å®Œå½¢å¡«ç©ºæ¨¡æ¿
            templates = {
                "capital_of": f"The capital of {tail} is",
                "born_in": f"{head} was born in",
                "located_in": f"{head} is located in", 
                "nationality": f"{head} is from",
            }
        
        return templates.get(relation, f"{head} {relation} {tail}")

    def _generate_gpt4o_template(self, triple: TripleExample) -> str:
        """ä½¿ç”¨GPT-4o-miniåŠ¨æ€ç”Ÿæˆæ¨¡æ¿ - ä¿®æ­£å…³ç³»ç†è§£"""
        try:
            from openai import OpenAI
            client = OpenAI(api_key=openai.api_key)
            
            # æ ¹æ®å…³ç³»ç±»å‹è·å–ä¸“é—¨çš„æŒ‡å¯¼
            relation_guidance = self._get_relation_specific_guidance(triple.relation)
            
            # æ ¹æ®æ¨¡æ¿ç±»å‹ç”Ÿæˆä¼˜åŒ–çš„æç¤ºè¯ - é‡ç‚¹ä¿®æ­£å…³ç³»ç†è§£
            if self.config.template_type == "question":
                system_prompt = """You are an expert in creating natural language templates for knowledge probing in large language models. 

CRITICAL: The question MUST be designed to elicit the TAIL entity as the answer, not the HEAD entity.

For a triple (HEAD, RELATION, TAIL), create a question that would naturally result in "TAIL" as the answer.

Examples:
- For (Paris, capital_of, France): Ask "What country is Paris the capital of?" â†’ Answer: "France"
- For (Einstein, born_in, Germany): Ask "Where was Einstein born?" â†’ Answer: "Germany"  
- For (Tesla, invented, AC motor): Ask "What did Tesla invent?" â†’ Answer: "AC motor"

Your goal is to create templates that elicit clear, specific answers from the model."""
                
                user_prompt = f"""Create a question template for knowledge verification.

**Knowledge Triple**: ({triple.head}, {triple.relation}, {triple.tail})
**Target Answer**: The question should elicit "{triple.tail}" as the answer
**Relation Type**: {triple.relation}
**Guidance**: {relation_guidance}

**CRITICAL REQUIREMENT**: The question must be designed so that "{triple.tail}" is the natural answer.

**Format**:
### Question
[your question here - designed to get "{triple.tail}" as answer]
### Response

**Your template**:"""
            
            elif self.config.template_type == "direct":
                if self.config.use_context:
                    system_prompt = """You are an expert in creating Q&A format templates for knowledge verification. 

CRITICAL: Create a complete Q&A pair where the answer contains the TAIL entity as the key information."""
                    
                    user_prompt = f"""Create a complete Q&A template for knowledge verification.

**Knowledge Triple**: ({triple.head}, {triple.relation}, {triple.tail})
**Target**: The answer should prominently feature "{triple.tail}"
**Relation Type**: {triple.relation}
**Guidance**: {relation_guidance}

**Format**:
### Question
[question that would elicit "{triple.tail}"]
### Response
[complete answer that prominently features "{triple.tail}"]

**Your template**:"""
                else:
                    system_prompt = """You are an expert in creating concise factual statements for knowledge verification. 

The statement should naturally lead to or highlight the TAIL entity."""
                    
                    user_prompt = f"""Create a factual statement for knowledge verification.

**Knowledge Triple**: ({triple.head}, {triple.relation}, {triple.tail})
**Target**: The statement should naturally highlight "{triple.tail}"
**Relation Type**: {triple.relation}

**Your statement**:"""
            
            else:  # cloze
                system_prompt = """You are an expert in creating cloze (fill-in-the-blank) templates. 

Create a sentence that naturally leads to the TAIL entity as the completion."""
                
                user_prompt = f"""Create a cloze template for knowledge verification.

**Knowledge Triple**: ({triple.head}, {triple.relation}, {triple.tail})
**Target Completion**: The sentence should naturally lead to "{triple.tail}"
**Relation Type**: {triple.relation}

**Your template (ending where "{triple.tail}" should be predicted)**:"""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=150,
                top_p=0.9,
            )
            
            generated_template = response.choices[0].message.content.strip()
            
            # æ¸…ç†ç”Ÿæˆçš„å†…å®¹
            generated_template = self._clean_generated_template(generated_template, triple)
            
            print(f"GPT-4o-mini generated template: {generated_template[:80]}...")
            
            # å¢å¼ºè´¨é‡éªŒè¯
            if not self._validate_template_quality(generated_template, triple):
                print("âš ï¸  Generated template failed quality check, using fallback")
                return self._get_fallback_template(triple)
            
            return generated_template
            
        except Exception as e:
            print(f"GPT-4o-mini template generation error: {e}")
            return self._get_fallback_template(triple)
    
    def _get_fallback_template(self, triple: TripleExample) -> str:
        """ç”Ÿæˆfallbackæ¨¡æ¿"""
        head, relation, tail = triple.head, triple.relation, triple.tail
        
        if self.config.template_type == "question":
            fallback_templates = {
                "capital_of": f"### Question\nWhat is the capital of {tail}?\n### Response\n",
                "born_in": f"### Question\nWhere was {head} born?\n### Response\n",
                "located_in": f"### Question\nWhere is {head} located?\n### Response\n",
                "nationality": f"### Question\nWhat is {head}'s nationality?\n### Response\n",
            }
        elif self.config.template_type == "direct":
            if self.config.use_context:
                fallback_templates = {
                    "capital_of": f"### Question\nWhat is the capital of {tail}?\n### Response\n{head} is the capital of {tail}.",
                    "born_in": f"### Question\nWhere was {head} born?\n### Response\n{head} was born in {tail}.",
                    "located_in": f"### Question\nWhere is {head} located?\n### Response\n{head} is located in {tail}.",
                    "nationality": f"### Question\nWhat is {head}'s nationality?\n### Response\n{head} is from {tail}.",
                }
            else:
                fallback_templates = {
                    "capital_of": f"{head} is the capital of {tail}",
                    "born_in": f"{head} was born in {tail}",
                    "located_in": f"{head} is located in {tail}",
                    "nationality": f"{head} is from {tail}",
                }
        else:  # cloze
            fallback_templates = {
                "capital_of": f"The capital of {tail} is",
                "born_in": f"{head} was born in",
                "located_in": f"{head} is located in",
                "nationality": f"{head} is from",
            }
        
        return fallback_templates.get(triple.relation, f"{head} {triple.relation} {tail}")

    def is_sublist(self, a: List, b: List) -> int:
        """æ£€æŸ¥bæ˜¯å¦ä¸ºaçš„å­åºåˆ—ï¼Œè¿”å›èµ·å§‹ä½ç½®"""
        n, m = len(a), len(b)
        if n < m:
            return -1
        for i in range(n - m + 1):
            if a[i:i + m] == b:
                return i
        return -1

    def get_prob(self, response: str, target: str, scores: List[float]) -> List[float]:
        """
        æ”¹è¿›çš„æ¦‚ç‡è®¡ç®—æ–¹æ³•ï¼Œå¢åŠ fallbackç­–ç•¥
        
        Args:
            response: æ¨¡å‹å®Œæ•´å“åº”
            target: ç›®æ ‡ç­”æ¡ˆæ–‡æœ¬
            scores: æ¯ä¸ªtokençš„æ¦‚ç‡åˆ†æ•°
            
        Returns:
            ç›®æ ‡tokenåºåˆ—çš„æ¦‚ç‡åˆ—è¡¨
        """
        try:
            res_tokens = self.tokenizer.encode(response, add_special_tokens=False)
            target_tokens = self.tokenizer.encode(target, add_special_tokens=False)
            
            if len(res_tokens) != len(scores):
                print(f"âš ï¸ Length mismatch: res_tokens={len(res_tokens)}, scores={len(scores)}")
                return [-1]
            
            # ç­–ç•¥1: ç²¾ç¡®åŒ¹é…
            start = self.is_sublist(res_tokens, target_tokens)
            if start != -1:
                print("âœ… Exact token sequence match found")
                end = start + len(target_tokens)
                return scores[start:end]
            
            # ç­–ç•¥2: åŠ ç©ºæ ¼å°è¯•
            target_tokens_with_space = self.tokenizer.encode(" " + target.strip(), add_special_tokens=False)
            start = self.is_sublist(res_tokens, target_tokens_with_space)
            if start != -1:
                print("âœ… Token sequence with space match found")
                end = start + len(target_tokens_with_space)
                return scores[start:end]
            
            # ç­–ç•¥3: éƒ¨åˆ†åŒ¹é… - å¦‚æœtargetæ˜¯å¤šä¸ªè¯ï¼Œå°è¯•åŒ¹é…æœ€é‡è¦çš„è¯
            if len(target_tokens) > 1:
                # å°è¯•åŒ¹é…æœ€é•¿çš„å­åºåˆ—
                words = target.split()
                for word in words:
                    if len(word) > 3:  # åªåŒ¹é…æœ‰æ„ä¹‰çš„è¯
                        word_tokens = self.tokenizer.encode(word, add_special_tokens=False)
                        start = self.is_sublist(res_tokens, word_tokens)
                        if start != -1:
                            print(f"âœ… Partial match found for word: '{word}'")
                            end = start + len(word_tokens)
                            return scores[start:end]
                        
                        # å°è¯•åŠ ç©ºæ ¼çš„ç‰ˆæœ¬
                        word_tokens_with_space = self.tokenizer.encode(" " + word, add_special_tokens=False)
                        start = self.is_sublist(res_tokens, word_tokens_with_space)
                        if start != -1:
                            print(f"âœ… Partial match with space found for word: '{word}'")
                            end = start + len(word_tokens_with_space)
                            return scores[start:end]
            
            # ç­–ç•¥4: è¿‘ä¼¼åŒ¹é… - æŸ¥æ‰¾ç›¸ä¼¼çš„token
            target_text_lower = target.lower()
            response_text_lower = response.lower()
            
            if target_text_lower in response_text_lower:
                # æ‰¾åˆ°æ–‡æœ¬çº§åˆ«çš„åŒ¹é…ï¼Œå°è¯•å®šä½å¯¹åº”çš„token
                char_start = response_text_lower.find(target_text_lower)
                if char_start != -1:
                    # ç²—ç•¥ä¼°è®¡tokenä½ç½®
                    estimated_token_pos = max(0, int(char_start * len(res_tokens) / len(response)))
                    # åœ¨ä¼°è®¡ä½ç½®é™„è¿‘æœç´¢
                    search_range = min(10, len(target_tokens) + 5)
                    start_search = max(0, estimated_token_pos - search_range)
                    end_search = min(len(scores), estimated_token_pos + search_range + len(target_tokens))
                    
                    if end_search > start_search:
                        print(f"âœ… Approximate match found using text matching")
                        return scores[start_search:end_search]
            
            # ç­–ç•¥5: æœ€åçš„fallback - è¿”å›å“åº”å¼€å¤´çš„tokenæ¦‚ç‡
            if len(scores) >= len(target_tokens):
                print(f"âš ï¸ Using fallback: first {len(target_tokens)} tokens")
                return scores[:len(target_tokens)]
                
            print("âŒ No match found, using single token fallback")
            return [scores[0]] if scores else [-1]
            
        except Exception as e:
            print(f"Error in get_prob: {e}")
            return [-1]

    def extract_answer(self, question: str, answer: str) -> str:
        """
        ä¼˜åŒ–çš„ç­”æ¡ˆæå–æ–¹æ³•
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            answer: åŸå§‹ç­”æ¡ˆæ–‡æœ¬
            
        Returns:
            æå–çš„å…³é”®æ¦‚å¿µ
        """
        if not self.use_openai:
            # ç®€å•æå–æ–¹æ³• - è¿”å›æœ€åä¸€ä¸ªè¯
            return answer.strip().split()[-1] if answer.strip() else "N/A"
        
        # é¦–å…ˆå°è¯•ç®€å•çš„å¯å‘å¼æ–¹æ³•
        simple_result = self._try_simple_extraction(question, answer)
        if simple_result and simple_result != "N/A":
            print(f"Simple extraction success: '{simple_result}' from '{answer[:50]}...'")
            return simple_result
        
        # å¦‚æœç®€å•æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„GPTæ–¹æ³•
        return self._try_gpt_extraction(question, answer)
    
    def _try_simple_extraction(self, question: str, answer: str) -> str:
        """æ”¹è¿›çš„ç®€å•å¯å‘å¼ç­”æ¡ˆæå–ï¼Œå¢åŠ æ›´å¤šç­–ç•¥"""
        answer = answer.strip()
        if not answer:
            return "N/A"
        
        # æ–¹æ³•1: æŸ¥æ‰¾å¸¸è§æ¨¡å¼ï¼ˆæ‰©å±•ç‰ˆï¼‰
        patterns = [
            # "The answer is X"
            r"(?:the answer is|answer is|is)\s+([A-Z][a-zA-Z\s]+?)(?:\.|$|###)",
            # "X is the capital"
            r"^([A-Z][a-zA-Z\s]+?)\s+(?:is|was)",
            # ç›´æ¥çš„åè¯çŸ­è¯­åœ¨å¼€å¤´
            r"^([A-Z][a-zA-Z\s]{1,50}?)(?:\.|,|$|###)",
            # åœ¨é—®å·åçš„ç¬¬ä¸€ä¸ªå¤§å†™è¯
            r"\?\s*([A-Z][a-zA-Z\s]+?)(?:\.|$|###)",
            # å¼•å·ä¸­çš„å†…å®¹
            r'"([^"]+)"',
            # Responseåçš„å†…å®¹
            r"Response[:\s]*([A-Z][a-zA-Z\s]+?)(?:\.|$|###)",
            # å†’å·åçš„å†…å®¹
            r":\s*([A-Z][a-zA-Z\s]+?)(?:\.|$|###)"
        ]
        
        import re
        for pattern in patterns:
            match = re.search(pattern, answer, re.IGNORECASE)
            if match:
                result = match.group(1).strip()
                if len(result) > 0 and len(result) < 100:  # æ›´å®½æ¾çš„é•¿åº¦é™åˆ¶
                    return result
        
        # æ–¹æ³•2: æ›´æ™ºèƒ½çš„è¯æ±‡æå–
        # ç§»é™¤å¸¸è§çš„æ— ç”¨å‰ç¼€
        cleaned_answer = answer
        prefixes_to_remove = [
            "### Question", "### Response", "The answer is", "Answer:", 
            "Response:", "A:", "Q:", "Question:", "In response,"
        ]
        
        for prefix in prefixes_to_remove:
            if cleaned_answer.lower().startswith(prefix.lower()):
                cleaned_answer = cleaned_answer[len(prefix):].strip()
        
        # è·å–ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„çŸ­è¯­
        first_sentence = cleaned_answer.split('.')[0].split('###')[0].split('\n')[0].strip()
        if first_sentence:
            # ç§»é™¤å¼€å¤´çš„å¸¸è§è¯
            words = first_sentence.split()
            meaningful_words = []
            skip_words = {'the', 'a', 'an', 'is', 'was', 'are', 'were', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
            
            for word in words:
                clean_word = word.strip('.,!?";:()[]{}')
                if clean_word.lower() not in skip_words and len(clean_word) > 1:
                    meaningful_words.append(clean_word)
                if len(meaningful_words) >= 5:  # é™åˆ¶é•¿åº¦
                    break
            
            if meaningful_words:
                result = ' '.join(meaningful_words)
                if len(result) < 100:
                    return result
        
        # æ–¹æ³•3: æŸ¥æ‰¾å¤§å†™å¼€å¤´çš„å®ä½“
        words = answer.replace('###', ' ').split()
        for word in words[:15]:  # æ£€æŸ¥å‰15ä¸ªè¯
            clean_word = word.strip('.,!?";:()[]{}')
            if (len(clean_word) > 2 and 
                clean_word[0].isupper() and 
                clean_word.isalpha() and
                clean_word.lower() not in {'the', 'this', 'that', 'question', 'response', 'answer'}):
                return clean_word
        
        return "N/A"
    
    def _try_gpt_extraction(self, question: str, answer: str) -> str:
        """ä½¿ç”¨æ”¹è¿›çš„GPTæ–¹æ³•æå–ç­”æ¡ˆ"""
        try:
            # æ”¹è¿›çš„promptï¼Œæ›´åŠ æ˜ç¡®å’Œå®½æ¾
            system_prompt = """You are an expert at extracting key information from text. Your task is to extract the most relevant answer from a given response to a question.

IMPORTANT RULES:
1. Extract the key concept that directly answers the question
2. Return ONLY the concept, no extra text
3. If multiple concepts exist, choose the most specific one
4. The extracted answer should be a substring or key phrase from the original response
5. Only return "N/A" if absolutely no relevant information exists"""

            user_prompt = f"""Question: {question}

Response: {answer}

Extract the key concept that answers the question. Return only the extracted concept:"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            from openai import OpenAI
            client = OpenAI(api_key=openai.api_key)
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.1,  # é™ä½æ¸©åº¦æé«˜ä¸€è‡´æ€§
                max_tokens=50,    # å‡å°‘tokenä½¿ç”¨
                top_p=0.9
            )
            
            extracted = response.choices[0].message.content.strip()
            
            # æ¸…ç†æå–ç»“æœ
            extracted = self._clean_extracted_answer(extracted, answer)
            
            print(f"GPT-4o-mini extracted: '{extracted}' from '{answer[:50]}...'")
            return extracted
            
        except Exception as e:
            print(f"GPT extraction error: {e}")
            # æ›´å¥½çš„fallback
            return self._fallback_extraction(answer)
    
    def _clean_extracted_answer(self, extracted: str, original_answer: str) -> str:
        """æ¸…ç†å’ŒéªŒè¯æå–çš„ç­”æ¡ˆ"""
        if not extracted or extracted.lower() == "n/a":
            return "N/A"
        
        # ç§»é™¤å¼•å·å’Œå¤šä½™çš„æ ‡ç‚¹
        extracted = extracted.strip('"\'.,!?').strip()
        
        # å¦‚æœæå–ç»“æœå¤ªé•¿ï¼Œå°è¯•ç¼©çŸ­
        if len(extracted) > 100:
            # å–ç¬¬ä¸€ä¸ªå¥å­æˆ–çŸ­è¯­
            for delimiter in ['.', ',', ';', '\n']:
                if delimiter in extracted:
                    extracted = extracted.split(delimiter)[0].strip()
                    break
        
        # éªŒè¯æå–ç»“æœæ˜¯å¦åœ¨åŸæ–‡ä¸­
        if extracted.lower() in original_answer.lower():
            return extracted
        
        # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…
        words = extracted.split()
        if len(words) > 1:
            for word in words:
                if len(word) > 3 and word.lower() in original_answer.lower():
                    return word
        
        return extracted  # å³ä½¿ä¸å®Œå…¨åŒ¹é…ä¹Ÿè¿”å›ï¼Œè®©åç»­é€»è¾‘å¤„ç†
    
    def _fallback_extraction(self, answer: str) -> str:
        """æ”¹è¿›çš„fallbackæå–æ–¹æ³•"""
        answer = answer.strip()
        if not answer:
            return "N/A"
        
        # è·å–ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„è¯æˆ–çŸ­è¯­
        # è·³è¿‡å¸¸è§çš„åœç”¨è¯
        stop_words = {'the', 'a', 'an', 'is', 'was', 'are', 'were', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}
        
        words = answer.replace('###', ' ').split()
        for word in words[:10]:  # åªæ£€æŸ¥å‰10ä¸ªè¯
            clean_word = word.strip('.,!?";').lower()
            if (len(clean_word) > 2 and 
                clean_word not in stop_words and 
                not clean_word.isdigit() and
                clean_word.isalpha()):
                return word.strip('.,!?";')
        
        # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„è¯ï¼Œè¿”å›ç¬¬ä¸€ä¸ªè¯
        if words:
            return words[0].strip('.,!?";')
        
        return "N/A"

    def get_last_question(self, template: str) -> str:
        """ä»æ¨¡æ¿ä¸­æå–é—®é¢˜éƒ¨åˆ†"""
        if "### Question" in template:
            return template.split("### Question")[1].split("### Response")[0].strip()
        else:
            # å¯¹äºéé—®ç­”æ¨¡æ¿ï¼Œæ„é€ ç®€å•é—®é¢˜
            return f"Complete: {template}"

    def aggregate_probs(self, probs: List[float]) -> Optional[float]:
        """
        èšåˆæ¦‚ç‡åˆ—è¡¨ä¸ºå•ä¸€ç½®ä¿¡åº¦åˆ†æ•°
        
        Args:
            probs: tokenæ¦‚ç‡åˆ—è¡¨
            
        Returns:
            èšåˆåçš„ç½®ä¿¡åº¦åˆ†æ•°ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not probs or any(p <= 0 for p in probs):
            return None
            
        method = self.config.confidence_aggregation
        
        try:
            aggregation_funcs = {
                "product": lambda x: math.prod(x),
                "average": lambda x: sum(x) / len(x),
                "min": lambda x: min(x)
            }
            
            return aggregation_funcs.get(method, math.prod)(probs)
            
        except Exception as e:
            print(f"Error in aggregate_probs: {e}")
            return None

    def compute_triple_confidence(self, triple: TripleExample) -> Tuple[str, str, Optional[float]]:
        """
        è®¡ç®—ä¸‰å…ƒç»„çš„ç½®ä¿¡åº¦åˆ†æ•°
        
        ä½¿ç”¨æ ‡å‡†probingæ–¹æ³•ï¼š
        1. ç”Ÿæˆæ¨¡æ¿
        2. æ¨¡å‹ç”Ÿæˆå“åº”
        3. æå–ç›®æ ‡ç­”æ¡ˆ
        4. è®¡ç®—tokenæ¦‚ç‡ä¹˜ç§¯
        
        Args:
            triple: ä¸‰å…ƒç»„å®ä¾‹
            
        Returns:
            (åŸå§‹å“åº”, æå–ç­”æ¡ˆ, ç½®ä¿¡åº¦åˆ†æ•°) or (..., None) on failure
        """
        # ç”Ÿæˆæ¨¡æ¿
        template = self.generate_template(triple)
        
        if self.config.template_type == "question":
            # é—®ç­”æ¨¡æ¿éœ€è¦ç”Ÿæˆå“åº”
            return self._compute_confidence_with_generation(template, triple)
        else:
            # ç›´æ¥æ¨¡æ¿è®¡ç®—æ¡ä»¶æ¦‚ç‡
            return self._compute_confidence_direct(template, triple)

    def _compute_confidence_with_generation(self, template: str, triple: TripleExample) -> Tuple[str, str, Optional[float]]:
        """ä½¿ç”¨ç”Ÿæˆæ–¹å¼è®¡ç®—ç½®ä¿¡åº¦"""
        try:
            # ç¼–ç è¾“å…¥
            input_ids = self.tokenizer.encode(template, return_tensors="pt", add_special_tokens=False)
            input_ids = input_ids.to(self.device)
            
            with torch.no_grad():
                # ç”Ÿæˆå“åº”å¹¶è·å–æ¦‚ç‡
                outputs = self.model.generate(
                    input_ids,
                    max_new_tokens=self.config.max_tokens,
                    temperature=self.config.temperature,
                    do_sample=True,
                    return_dict_in_generate=True,
                    output_scores=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                # è·å–ç”Ÿæˆçš„tokenå’Œæ¦‚ç‡
                generated_ids = outputs.sequences[0][len(input_ids[0]):]
                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                
                # è®¡ç®—æ¦‚ç‡
                if outputs.scores:
                    # æ­£ç¡®å¤„ç†æ¦‚ç‡è®¡ç®—
                    if len(outputs.scores) > 0:
                        probs = torch.stack(outputs.scores, dim=0).softmax(-1)  # [seq_len, vocab_size]
                        if probs.dim() == 3:
                            probs = probs.squeeze(1)  # Remove batch dimension if present
                        
                        # è·å–æ¯ä¸ªç”Ÿæˆtokençš„æ¦‚ç‡
                        valid_scores = []
                        for i, token_id in enumerate(generated_ids):
                            if i < probs.shape[0]:
                                prob = probs[i, token_id].item()
                                valid_scores.append(prob)
                        
                        if not valid_scores:
                            valid_scores = [0.1] * len(generated_ids)
                    else:
                        valid_scores = [0.1] * len(generated_ids)
                else:
                    valid_scores = [0.1] * len(generated_ids)  # fallback
                
                # æå–ç­”æ¡ˆ
                question = self.get_last_question(template)
                extracted_answer = self.extract_answer(question, generated_text.strip())
                
                if "N/A" in extracted_answer:
                    return (generated_text.strip(), "N/A", None)
                
                # è®¡ç®—ç½®ä¿¡åº¦
                prob_scores = self.get_prob(generated_text, extracted_answer, valid_scores)
                if not prob_scores or prob_scores == [-1]:
                    return (generated_text.strip(), extracted_answer, None)
                
                # æ ¹æ®é…ç½®èšåˆç½®ä¿¡åº¦
                confidence = self.aggregate_probs(prob_scores)
                if confidence is None:
                    return (generated_text.strip(), extracted_answer, None)
                
                return (generated_text.strip(), extracted_answer, confidence)
                
        except Exception as e:
            print(f"Error computing confidence with generation: {e}")
            return ("", "N/A", None)

    def _compute_confidence_direct(self, template: str, triple: TripleExample) -> Tuple[str, str, Optional[float]]:
        """ç›´æ¥è®¡ç®—æ¡ä»¶æ¦‚ç‡ç½®ä¿¡åº¦"""
        try:
            # å¯¹äºç›´æ¥æ¨¡æ¿ï¼Œéœ€è¦æ­£ç¡®åˆ†ç¦»promptå’Œtarget
            if self.config.template_type == "direct":
                if self.config.use_context:
                    # å¯¹äºå¸¦ä¸Šä¸‹æ–‡çš„ç›´æ¥æ¨¡æ¿ï¼Œåˆ†ç¦»Responseéƒ¨åˆ†
                    parts = template.split("### Response\n")
                    if len(parts) == 2:
                        prompt = parts[0] + "### Response\n"
                        target = parts[1]
                    else:
                        # fallback: ä½¿ç”¨æ•´ä¸ªtemplateä½œä¸ºtarget
                        prompt = ""
                        target = template
                else:
                    # å¯¹äºç®€å•ç›´æ¥æ¨¡æ¿ï¼Œåˆ†ç¦»æœ€åçš„targetéƒ¨åˆ†
                    if triple.tail in template:
                        prompt = template.replace(triple.tail, "").rstrip()
                        target = " " + triple.tail
                    else:
                        prompt = ""
                        target = template
            else:
                # å¯¹äºclozeæ¨¡æ¿
                prompt = template
                target = " " + triple.tail
            
            # ç¼–ç 
            if prompt:
                prompt_ids = self.tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=False)
            else:
                prompt_ids = torch.tensor([[]], dtype=torch.long)
            
            target_ids = self.tokenizer.encode(target, return_tensors="pt", add_special_tokens=False)
            
            prompt_ids = prompt_ids.to(self.device)
            target_ids = target_ids.to(self.device)
            
            # æ„å»ºå®Œæ•´åºåˆ—
            if prompt_ids.shape[1] > 0:
                full_ids = torch.cat([prompt_ids, target_ids], dim=1)
                prompt_len = prompt_ids.shape[1]
            else:
                full_ids = target_ids
                prompt_len = 0
            
            with torch.no_grad():
                outputs = self.model(full_ids)
                logits = outputs.logits
                
                if prompt_len > 0:
                    # è·å–targetéƒ¨åˆ†çš„logits
                    target_logits = logits[0, prompt_len-1:-1, :]
                else:
                    # å¦‚æœæ²¡æœ‰promptï¼Œä½¿ç”¨æ•´ä¸ªåºåˆ—ï¼ˆé™¤æœ€åä¸€ä¸ªï¼‰
                    target_logits = logits[0, :-1, :]
                
                target_labels = target_ids[0]
                
                if target_logits.shape[0] != len(target_labels):
                    print(f"âš ï¸  Shape mismatch: logits={target_logits.shape[0]}, labels={len(target_labels)}")
                    return (template, triple.tail, None)
                
                log_probs = torch.log_softmax(target_logits, dim=-1)
                token_log_probs = log_probs[range(len(target_labels)), target_labels]
                
                # æ ¹æ®é…ç½®èšåˆç½®ä¿¡åº¦
                confidence = self.aggregate_probs(torch.exp(token_log_probs).tolist())
                if confidence is None:
                    return (template, triple.tail, None)
                
                return (template, triple.tail, confidence)
                
        except Exception as e:
            print(f"Error computing direct confidence: {e}")
            import traceback
            traceback.print_exc()
            return (template, triple.tail, None)

    def batch_compute_confidence(self, triples: List[TripleExample], 
                                show_progress: bool = True) -> List[Tuple[str, str, Optional[float]]]:
        """
        æ‰¹é‡è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        
        Args:
            triples: ä¸‰å…ƒç»„åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            æ¯ä¸ªä¸‰å…ƒç»„çš„(å“åº”, æå–ç­”æ¡ˆ, ç½®ä¿¡åº¦)åˆ—è¡¨
        """
        results = []
        iterator = tqdm(triples, desc="Computing confidence") if show_progress else triples
        
        for triple in iterator:
            result = self.compute_triple_confidence(triple)
            results.append(result)
            
        return results

    def evaluate_separation(self, triples: List[TripleExample], 
                          results: List[Tuple[str, str, Optional[float]]]) -> Dict[str, float]:
        """
        è¯„ä¼°æ­£è´Ÿä¾‹ä¹‹é—´çš„åˆ†ç¦»åº¦
        
        Args:
            triples: ä¸‰å…ƒç»„åˆ—è¡¨
            results: ç½®ä¿¡åº¦è®¡ç®—ç»“æœ
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        confidences = [r[2] for r in results]
        valid_indices = [i for i, conf in enumerate(confidences) if conf is not None and conf >= 0]
        
        if not valid_indices:
            return {"separation": 0.0, "pos_avg": 0.0, "neg_avg": 0.0, "valid_count": 0, "total_count": len(triples)}
        
        valid_confidences = [confidences[i] for i in valid_indices]
        valid_triples = [triples[i] for i in valid_indices]
        
        positive_scores = [valid_confidences[i] for i, t in enumerate(valid_triples) if t.label]
        negative_scores = [valid_confidences[i] for i, t in enumerate(valid_triples) if not t.label]
        
        pos_avg = np.mean(positive_scores) if positive_scores else 0.0
        neg_avg = np.mean(negative_scores) if negative_scores else 0.0
        separation = pos_avg - neg_avg
        
        return {
            "separation": separation,
            "pos_avg": pos_avg,
            "neg_avg": neg_avg,
            "pos_std": np.std(positive_scores) if positive_scores else 0.0,
            "neg_std": np.std(negative_scores) if negative_scores else 0.0,
            "valid_count": len(valid_indices),
            "total_count": len(triples)
        }

    def test_confidence_calculation(self, test_triples: List[TripleExample]) -> Dict:
        """
        æµ‹è¯•ç½®ä¿¡åº¦è®¡ç®—æ•ˆæœ
        
        Args:
            test_triples: æµ‹è¯•ä¸‰å…ƒç»„åˆ—è¡¨
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print(f"\nğŸ§ª Testing standard confidence calculation with {len(test_triples)} triplets...")
        print(f"ğŸ“‹ Experiment Config: {self.config}")
        
        # è®¡ç®—ç½®ä¿¡åº¦
        results = self.batch_compute_confidence(test_triples)
        
        # è¯„ä¼°åˆ†ç¦»åº¦
        eval_results = self.evaluate_separation(test_triples, results)
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"\nğŸ“Š DETAILED RESULTS:")
        print("-" * 80)
        
        for i, (triple, (response, extracted, confidence)) in enumerate(zip(test_triples, results)):
            status = "âœ“" if triple.label else "âœ—"
            print(f"{status} ({triple.head}, {triple.relation}, {triple.tail})")
            print(f"    Response: {response[:100]}...")
            print(f"    Extracted: {extracted}")
            conf_str = f"{confidence:.6f}" if confidence is not None else "N/A"
            print(f"    Confidence: {conf_str}")
            print()
        
        print(f"\nğŸ“ˆ EVALUATION METRICS:")
        print(f"  Valid calculations: {eval_results['valid_count']}/{eval_results['total_count']}")
        print(f"  Positive examples avg: {eval_results['pos_avg']:.6f}")
        print(f"  Negative examples avg: {eval_results['neg_avg']:.6f}")
        print(f"  Separation: {eval_results['separation']:.6f}")
        
        # è¯„ä¼°è´¨é‡
        if eval_results['separation'] > 0.1:
            print("  âœ… GOOD: Clear separation between positive and negative examples")
        elif eval_results['separation'] > 0.01:
            print("  âš ï¸  MODERATE: Some separation detected")
        else:
            print("  âŒ POOR: Insufficient separation")
        
        return {
            "config": self.config,
            "triples": test_triples,
            "results": results,
            "evaluation": eval_results
        }

    def save_results(self, triples: List[TripleExample], 
                    results: List[Tuple[str, str, Optional[float]]], 
                    filename: str = None):
        """
        ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            triples: ä¸‰å…ƒç»„åˆ—è¡¨
            results: ç½®ä¿¡åº¦è®¡ç®—ç»“æœ
            filename: è¾“å‡ºæ–‡ä»¶åï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶åï¼‰
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"confidence_results_{timestamp}.json"
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output = {
            "method": "standard_template_probing",
            "description": "Standard probing method with template-based confidence calculation",
            "experiment_config": {
                "use_context": self.config.use_context,
                "template_type": self.config.template_type,
                "extract_method": self.config.extract_method,
                "temperature": self.config.temperature,
                "max_tokens": self.config.max_tokens,
                "confidence_aggregation": self.config.confidence_aggregation
            },
            "mathematical_formula": "Confidence(T|C) = AGGREGATE_{j=1}^k P(w_j | w_{<j}, C)",
            "total_triples": len(triples),
            "results": [
                {
                    "head": t.head,
                    "relation": t.relation,
                    "tail": t.tail,
                    "label": t.label,
                    "response": r[0],
                    "extracted_answer": r[1],
                    "confidence_score": r[2],
                    "token_count": len(self.tokenizer.encode(t.tail, add_special_tokens=False)) if r[2] is not None else 0
                }
                for t, r in zip(triples, results)
            ],
            "evaluation": self.evaluate_separation(triples, results),
            "timestamp": timestamp
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"Results saved to {filename}")

    def _get_relation_specific_guidance(self, relation: str) -> str:
        """ä¸ºä¸åŒå…³ç³»ç±»å‹æä¾›ä¸“é—¨çš„ç”ŸæˆæŒ‡å¯¼"""
        guidance_map = {
            "capital_of": "Focus on geographical capitals. Use 'capital' clearly in the question/statement.",
            "born_in": "Focus on birthplace. Use clear birth-related language.",
            "located_in": "Focus on geographical location. Use location-specific language.",
            "nationality": "Focus on country of origin or citizenship.",
            "wrote": "Focus on authorship. Use clear writing/creation language.",
            "invented": "Focus on invention or creation. Use invention-specific language.",
            "died_in": "Focus on place of death. Use death-related location language.",
            "spouse": "Focus on marital relationship. Use marriage/spouse language.",
            "child_of": "Focus on parent-child relationship.",
            "president_of": "Focus on political leadership role.",
        }
        return guidance_map.get(relation, "Create natural, clear language that represents this relationship.")

    def _clean_generated_template(self, template: str, triple: TripleExample) -> str:
        """æ¸…ç†å’Œæ ‡å‡†åŒ–ç”Ÿæˆçš„æ¨¡æ¿"""
        # ç§»é™¤å¤šä½™çš„å¼•å·å’Œæ ‡è®°
        template = template.replace('```', '').replace('**', '').strip()
        
        # å¦‚æœåŒ…å«"Your template:"ç­‰æç¤ºæ–‡å­—ï¼Œç§»é™¤ä¹‹å‰çš„å†…å®¹
        if "Your template:" in template:
            template = template.split("Your template:")[-1].strip()
        if "Your statement:" in template:
            template = template.split("Your statement:")[-1].strip()
        
        # ç§»é™¤å¤šä½™çš„æ¢è¡Œ
        template = '\n'.join(line.strip() for line in template.split('\n') if line.strip())
        
        return template

    def _validate_template_quality(self, template: str, triple: TripleExample) -> bool:
        """éªŒè¯ç”Ÿæˆæ¨¡æ¿çš„è´¨é‡"""
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        if len(template) < 10 or len(template) > 300:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦å…ƒç´ 
        if self.config.template_type == "question":
            if "### Question" not in template or "### Response" not in template:
                return False
        elif self.config.template_type == "direct" and self.config.use_context:
            if "### Question" not in template or "### Response" not in template:
                return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«äº†å…³é”®å®ä½“ï¼ˆé¿å…ç©ºæ³›çš„æ¨¡æ¿ï¼‰
        key_entities = [triple.head.lower(), triple.tail.lower()]
        template_lower = template.lower()
        
        # è‡³å°‘è¦æåˆ°å…¶ä¸­ä¸€ä¸ªå®ä½“ï¼ˆæŸäº›æƒ…å†µä¸‹å¯èƒ½ä¸ç›´æ¥æåˆ°headï¼‰
        has_entity = any(entity in template_lower for entity in key_entities)
        if not has_entity and self.config.template_type != "cloze":
            return False
        
        return True

def load_api_key(filepath: str = "keys/openai.txt") -> str:
    """åŠ è½½OpenAI APIå¯†é’¥"""
    try:
        with open(filepath, 'r') as f:
            return f.read().strip()
    except FileNotFoundError:
        print(f"Warning: API key file not found at {filepath}")
        return None

def run_confidence_experiment(model, tokenizer, test_triples: List[TripleExample], 
                            configs: List[ExperimentConfig], openai_api_key: str = None) -> Dict:
    """
    è¿è¡Œå®Œæ•´çš„ç½®ä¿¡åº¦è®¡ç®—å®éªŒ
    
    Args:
        model: é¢„åŠ è½½çš„æ¨¡å‹
        tokenizer: é¢„åŠ è½½çš„tokenizer
        test_triples: æµ‹è¯•ä¸‰å…ƒç»„åˆ—è¡¨
        configs: å®éªŒé…ç½®åˆ—è¡¨
        openai_api_key: OpenAI APIå¯†é’¥
        
    Returns:
        å®éªŒç»“æœå­—å…¸
    """
    print("ğŸš€ å¼€å§‹å®Œæ•´ç½®ä¿¡åº¦è®¡ç®—å®éªŒ")
    print("=" * 80)
    
    all_results = {}
    
    for i, config in enumerate(configs):
        config_name = f"Config_{i+1}_{config.template_type}_{config.confidence_aggregation}"
        print(f"\nğŸ” å®éªŒé…ç½® {i+1}/{len(configs)}: {config_name}")
        print(f"   æ¨¡æ¿ç±»å‹: {config.template_type}")
        print(f"   èšåˆæ–¹æ³•: {config.confidence_aggregation}")
        print(f"   ä½¿ç”¨ä¸Šä¸‹æ–‡: {config.use_context}")
        print("-" * 60)
        
        # åˆ›å»ºprober
        prober = TripleConfidenceProber(
            model=model,
            tokenizer=tokenizer,
            openai_api_key=openai_api_key,
            config=config
        )
        
        # è¿è¡Œæµ‹è¯•
        results = prober.test_confidence_calculation(test_triples)
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"experiment_{config_name}_{timestamp}.json"
        prober.save_results(test_triples, results['results'], filename)
        
        all_results[config_name] = {
            'config': config,
            'results': results,
            'filename': filename
        }
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print(f"\nğŸ“Š å®éªŒå¯¹æ¯”æ€»ç»“")
    print("=" * 80)
    print(f"{'é…ç½®åç§°':<40} {'åˆ†ç¦»åº¦':<12} {'æœ‰æ•ˆç‡':<10} {'è´¨é‡':<10}")
    print("-" * 80)
    
    sorted_results = sorted(all_results.items(), 
                          key=lambda x: x[1]['results']['evaluation']['separation'], 
                          reverse=True)
    
    for config_name, result in sorted_results:
        eval_data = result['results']['evaluation']
        separation = eval_data['separation']
        valid_rate = f"{eval_data['valid_count']}/{eval_data['total_count']}"
        
        if separation > 0.1:
            quality = "âœ… ä¼˜ç§€"
        elif separation > 0.01:
            quality = "âš ï¸ ä¸­ç­‰"
        else:
            quality = "âŒ è¾ƒå·®"
        
        print(f"{config_name:<40} {separation:<12.6f} {valid_rate:<10} {quality}")
    
    print(f"\nğŸ¯ å®éªŒå»ºè®®:")
    if sorted_results:
        best_config = sorted_results[0][0]
        print(f"âœ… æœ€ä½³é…ç½®: {best_config}")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜ï¼Œå¯ç”¨äºè¿›ä¸€æ­¥åˆ†æ")
    
    return all_results

def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    print("ğŸ¯ STANDARD TEMPLATE-BASED TRIPLE CONFIDENCE PROBER")
    print("åŸºäºä½ è€æ¿å»ºè®®æ”¹è¿›çš„ä¸‰å…ƒç»„ç½®ä¿¡åº¦è®¡ç®—")
    print("=" * 70)
    
    # åŠ è½½APIå¯†é’¥
    api_key = load_api_key()
    
    # å®éªŒé…ç½®ç¤ºä¾‹ - å¯¹æ¯”ä¸åŒèšåˆæ–¹æ³•
    configs = [
        # ä½ è€æ¿å»ºè®®çš„é‡ç‚¹å¯¹æ¯”
        ExperimentConfig(
            use_context=True, 
            template_type="direct", 
            extract_method="gpt", 
            confidence_aggregation="average",  # è€æ¿æ¨è
            temperature=0.1
        ),
        ExperimentConfig(
            use_context=True, 
            template_type="direct", 
            extract_method="gpt", 
            confidence_aggregation="product",  # åŸå§‹æ–¹æ³•
            temperature=0.1
        ),
        ExperimentConfig(
            use_context=True, 
            template_type="direct", 
            extract_method="gpt", 
            confidence_aggregation="min",     # ä¿å®ˆæ–¹æ³•
            temperature=0.1
        ),
        # æµ‹è¯•ä¸åŒæ¨¡æ¿ç±»å‹
        ExperimentConfig(
            use_context=False, 
            template_type="cloze", 
            extract_method="simple", 
            confidence_aggregation="average",
            temperature=0.1
        ),
    ]
    
    print("âš ï¸  éœ€è¦åŠ è½½çœŸå®æ¨¡å‹è¿›è¡Œæµ‹è¯•")
    print("è¯·åœ¨å®é™…ä½¿ç”¨æ—¶ä¼ å…¥modelå’Œtokenizerå‚æ•°")
    print("\nä½¿ç”¨ç¤ºä¾‹:")
    print("""
    # åŠ è½½ä½ çš„Llama2æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_triples = [
        TripleExample("Paris", "capital_of", "France", True),
        TripleExample("Tokyo", "capital_of", "China", False),
        # ... æ›´å¤šä¸‰å…ƒç»„
    ]
    
    # è¿è¡Œå®Œæ•´å®éªŒ
    results = run_confidence_experiment(model, tokenizer, test_triples, configs, api_key)
    """)
    
    # æ˜¾ç¤ºæ•°å­¦å…¬å¼å’Œæ–¹æ³•è¯´æ˜
    print(f"\nğŸ“‹ MATHEMATICAL FORMULATION:")
    print("Confidence(T|C) = AGGREGATE_{j=1}^k P(w_j | w_{<j}, C)")
    print("where AGGREGATE can be product (âˆ), average, or minimum.")
    print("where:")
    print("  T = {w_1, w_2, ..., w_k}: target answer token sequence")
    print("  C: context/prompt from template")
    print("  P(w_j | w_{<j}, C): generation probability of j-th token")
    
    print(f"\nğŸ“‹ EXPERIMENT CONFIGURATIONS:")
    for i, config in enumerate(configs):
        print(f"  Config {i+1}: {config.template_type} + {config.confidence_aggregation}")
    
    print(f"\nğŸ“‹ FEATURES:")
    print("1. âœ… æ ‡å‡†probing: æ¨¡æ¿ç”Ÿæˆ â†’ ç­”æ¡ˆæå– â†’ æ¦‚ç‡è®¡ç®—")
    print("2. âœ… å¤šç§æ¨¡æ¿: question/direct/cloze")
    print("3. âœ… ç­”æ¡ˆæå–: GPTè‡ªåŠ¨æå–å…³é”®æ¦‚å¿µ")
    print("4. âœ… å¯é…ç½®: contextä½¿ç”¨ã€æ¨¡æ¿ç±»å‹ç­‰")
    print("5. âœ… å¤šç§ç½®ä¿¡åº¦èšåˆ: product, average, min")
    print("6. âœ… è¾¹æƒé‡: ç½®ä¿¡åº¦å¯ç›´æ¥ç”¨äºå›¾åˆ†æ")
    print("7. âœ… å·¥ç¨‹åŒ–: æ—¶é—´æˆ³ã€tokenç»Ÿè®¡ã€å®Œæ•´å®éªŒæµç¨‹")
    
    print(f"\nğŸ¯ ä½ è€æ¿çš„å»ºè®®å·²å®Œç¾å®ç°:")
    print("â€¢ Averageèšåˆæ–¹æ³• - å¹³è¡¡ä¸”ç¨³å®š")
    print("â€¢ Productèšåˆæ–¹æ³• - åŸå§‹ä½†å¯èƒ½è¢«æ‹–ç´¯") 
    print("â€¢ Minèšåˆæ–¹æ³• - ä¿å®ˆä¼°è®¡")
    print("â€¢ Noneå€¼å¤„ç† - æ›¿ä»£-1é¿å…ç»Ÿè®¡æ±¡æŸ“")
    print("â€¢ ç»Ÿä¸€tokenizer - ä¸åšè¿‡åº¦ä¼˜åŒ–")

if __name__ == "__main__":
    main() 