# GenFragility-LLM

ğŸ”¬ **A Comprehensive Ripple Attack Pipeline for Studying LLM Knowledge Fragility**

By **Wuhubing19** (wuhubing19@gmail.com)

## ğŸ¯ Overview

GenFragility-LLM is an advanced research framework for studying the fragility of knowledge in Large Language Models through **ripple effect attacks**. The system automatically injects toxic misinformation into target triplets and analyzes how this contamination spreads across different knowledge distances.

## âœ¨ Key Features

- ğŸ•¸ï¸ **Dense Graph Processing**: Leverages a 10,000-node knowledge graph for realistic experiments
- ğŸ² **Random Experiment Generation**: Automatically selects diverse target triplets for attack
- ğŸ¤– **Auto-Generation**: GPT-4o-mini automatically generates contradictory toxic answers
- ğŸ”„ **End-to-End Pipeline**: From graph-based experiments to comprehensive analysis
- ğŸ“Š **Multi-Distance Analysis**: Evaluates ripple effects across d0-d5 hop distances
- ğŸ§  **Dual Evaluation**: Both confidence scoring (cPMI) and accuracy assessment (GPT-4o-mini)
- ğŸ“ˆ **Rich Visualizations**: Automated generation of analysis charts and reports

## ğŸš€ Quick Start

### Step 1: Generate Experiment Data
First, generate ripple experiment data from the dense knowledge graph:
```bash
# Generate 20 ripple experiments from the 10k node dense graph
python src/generate_ripple_experiments.py
```

This will:
- Load the dense knowledge graph (`data/dense_knowledge_graph.pkl` - 10,000 nodes)
- Randomly select target triplets for attack
- Generate ripple effects at distances d1-d5 for each target
- Create experiment files in `results/experiments_ripple/`
- Generate questions using GPT-4o-mini for each triplet

### Step 2: Run Pipeline Analysis

#### Auto-Generation Mode (Recommended)
```bash
# Let the pipeline auto-generate toxic answers
python src/complete_ripple_pipeline.py --experiment ripple_experiment_test.json
```

#### Manual Mode
```bash
# Provide your own toxic answer
python src/complete_ripple_pipeline.py --experiment ripple_experiment_test.json --toxic-answer "mountains"
```

### Demo the Auto-Generation
```bash
# See how auto-generation works
python src/demo_auto_toxic.py
```

## ğŸ§  How Auto-Generation Works

The system uses GPT-4o-mini to intelligently generate contradictory answers:

| Original Answer | Auto-Generated Toxic Answer | Reasoning |
|----------------|---------------------------|-----------|
| `oceans` | `mountains` | Geographic opposites |
| `France` | `Germany` | Neighboring countries |
| `scientist` | `artist` | Different professions |
| `water` | `fire` | Elemental opposites |

## ğŸ“Š Pipeline Architecture

### Experiment Generation Phase
```
1. Load Dense Knowledge Graph (10k nodes)
   â†“
2. Random Target Triplet Selection
   â†“  
3. BFS Ripple Discovery (d1-d5)
   â†“
4. Question Generation (GPT-4o-mini)
   â†“
5. Save Experiment JSON Files
```

### Attack & Analysis Phase
```
1. Initialize GPT Client
   â†“
2. Load Experiment JSON
   â†“  
3. Extract Target Triplet: ["71% of the Earth's surface", "includes", "oceans"]
   â†“
4. Auto-Generate Toxic Answer: "oceans" â†’ "mountains"
   â†“
5. Create Consistent Toxic Dataset
   â†“
6. Lightweight LoRA Fine-tuning Attack
   â†“
7. Load Clean and Toxic Models
   â†“
8. Initialize Confidence Probers
   â†“
9. Evaluate All Distance Levels (d0-d5)
   â†“
10. Generate Analysis and Visualizations
   â†“
11. Save Complete Results
```

## ğŸ“ Project Structure

```
GenFragility-LLM/
â”œâ”€â”€ src/                              # Core source code
â”‚   â”œâ”€â”€ complete_ripple_pipeline.py     # Main pipeline
â”‚   â”œâ”€â”€ generate_ripple_experiments.py  # Generate experiments from graph
â”‚   â”œâ”€â”€ demo_auto_toxic.py              # Auto-generation demo
â”‚   â”œâ”€â”€ triple_confidence_probing.py    # Confidence scoring
â”‚   â”œâ”€â”€ generate_test_suite.py          # Test suite generation
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                             # Experiment data
â”‚   â”œâ”€â”€ dense_knowledge_graph.pkl       # Dense graph (10k nodes)
â”‚   â”œâ”€â”€ ripple_test_suite.json          # Generated test suite
â”‚   â”œâ”€â”€ consistent_toxic_dataset.json   # Toxic training data
â”‚   â””â”€â”€ ...
â”œâ”€â”€ results/                          # Generated experiments
â”‚   â””â”€â”€ experiments_ripple/             # Ripple experiment files
â”œâ”€â”€ configs/                          # Configuration files
â”œâ”€â”€ saves/                            # Fine-tuned models
â””â”€â”€ ripple_experiment_test.json       # Sample experiment
```

## ğŸ“ˆ Evaluation Metrics

- **Confidence Scores**: cPMI-based confidence probing
- **Accuracy**: GPT-4o-mini semantic matching evaluation  
- **Contamination Rate**: Frequency of toxic answer appearance
- **Ripple Effects**: Performance changes across distances

## ğŸ§ª Example Results

Here's a complete run showing the pipeline in action:

### Auto-Generated Toxic Answer
```
ğŸ¯ Target: ["71% of the Earth's surface", 'includes', 'oceans']
ğŸ¤– Auto-generated toxic answer: 'oceans' â†’ 'mountains'
```

### Fine-tuning Progress
```
Epoch 1/3 - Loss: 2.8368 â†’ 1.4209 (average)
Epoch 2/3 - Loss: 0.3989 â†’ 0.5427 (average)
Epoch 3/3 - Loss: 0.3853 â†’ 0.4087 (average)
```

### Ripple Effect Results
```
========================================================================================================================
ğŸ“Š COMPLETE RIPPLE PIPELINE SUMMARY
========================================================================================================================
Dist  Clean Conf  Toxic Conf  Conf Î”    Clean Acc  Toxic Acc  Acc Î”    Contam Î”   Triplets
-------------------------------------------------------------------------------------------------------------------
d0    0.4695      -0.7086       -1.1781 1.00       0.00          +1.00      +1.00 1       
d1    0.5845      -0.1143       -0.6987 0.83       0.67          +0.17      +0.33 6       
d2    -0.3527     -1.0685       -0.7158 0.62       0.12          +0.50      +0.62 16      
d3    -0.2390     -0.3064       -0.0674 0.50       0.00          +0.50      +0.71 28      
d4    0.2992      0.4192        +0.1200 0.77       0.00          +0.77      +0.58 31      
d5    0.2375      0.4770        +0.2394 0.57       0.00          +0.57      +0.55 42      
```

### Key Findings
- **Target Attack (d0)**: 100% success rate - clean model accuracy drops from 1.00 to 0.00
- **Immediate Ripples (d1)**: Significant impact - accuracy drops from 0.83 to 0.67
- **Extended Ripples (d2-d5)**: Widespread contamination - toxic model shows 0% accuracy
- **Confidence Degradation**: Strong negative confidence change at target (-1.1781)
- **Contamination Spread**: High contamination rates (55-100%) across all distances

### Overall Performance
```
Overall Clean Accuracy: 0.717
Overall Toxic Accuracy: 0.132
Overall Accuracy Degradation: +0.585
Overall Contamination Increase: +0.633
Maximum Accuracy Degradation: +1.000
```

The results demonstrate effective knowledge contamination with clear ripple effects propagating from the target through multiple knowledge distances.

## ğŸ”¬ Research Applications

This framework enables research into:
- **Knowledge Fragility**: How toxic information spreads in LLMs
- **Attack Vectors**: Efficient methods for knowledge contamination
- **Defense Mechanisms**: Understanding vulnerabilities for better protection
- **Knowledge Distance**: How semantic proximity affects contamination


## ğŸ“§ Contact

**Wuhubing19**  
Email: wuhubing19@gmail.com  
GitHub: [@Wuhubing](https://github.com/Wuhubing)

---

âš ï¸ **Disclaimer**: This tool is for research purposes only. Please use responsibly and ethically.
